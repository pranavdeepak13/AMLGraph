{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d622ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2cd08bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5,078,345 transactions\n",
      "Loaded 518,581 accounts\n",
      "Loaded 370 patterns\n",
      "Loaded 3209 pattern transactions\n"
     ]
    }
   ],
   "source": [
    "transactions_df = pd.read_csv('../data/processed_transactions.csv')\n",
    "accounts_df = pd.read_csv('../data/processed_accounts.csv')\n",
    "patterns_df = pd.read_csv('../data/processed_patterns.csv')\n",
    "pattern_transactions_df = pd.read_csv('../data/processed_pattern_transactions.csv')\n",
    "\n",
    "transactions_df['timestamp'] = pd.to_datetime(transactions_df['timestamp'])\n",
    "pattern_transactions_df['timestamp'] = pd.to_datetime(pattern_transactions_df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(transactions_df):,} transactions\")\n",
    "print(f\"Loaded {len(accounts_df):,} accounts\")\n",
    "print(f\"Loaded {len(patterns_df)} patterns\")\n",
    "print(f\"Loaded {len(pattern_transactions_df)} pattern transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9440369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing overall dataset characteristics...\n",
      "Dataset Overview:\n",
      "  Total transactions: 5,078,345\n",
      "  Money laundering transactions: 5,177\n",
      "  Normal transactions: 5,073,168\n",
      "  ML rate: 0.001019\n",
      "  Unique accounts: 515,080\n",
      "  Unique banks: 30,470\n",
      "  Date range: 2022-09-01 00:00:00 to 2022-09-18 16:18:00\n",
      "  Analysis period: 17 days\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing overall dataset characteristics...\")\n",
    "\n",
    "total_transactions = len(transactions_df)\n",
    "total_accounts = len(set(transactions_df['account_origin'].unique()) | set(transactions_df['account_destination'].unique()))\n",
    "total_banks = len(set(transactions_df['from_bank'].unique()) | set(transactions_df['to_bank'].unique()))\n",
    "\n",
    "ml_transactions = transactions_df[transactions_df['is_laundering'] == 1]\n",
    "normal_transactions = transactions_df[transactions_df['is_laundering'] == 0]\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"  Total transactions: {total_transactions:,}\")\n",
    "print(f\"  Money laundering transactions: {len(ml_transactions):,}\")\n",
    "print(f\"  Normal transactions: {len(normal_transactions):,}\")\n",
    "print(f\"  ML rate: {len(ml_transactions)/total_transactions:.6f}\")\n",
    "print(f\"  Unique accounts: {total_accounts:,}\")\n",
    "print(f\"  Unique banks: {total_banks:,}\")\n",
    "print(f\"  Date range: {transactions_df['timestamp'].min()} to {transactions_df['timestamp'].max()}\")\n",
    "\n",
    "time_span = transactions_df['timestamp'].max() - transactions_df['timestamp'].min()\n",
    "print(f\"  Analysis period: {time_span.days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21779ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing temporal patterns...\n",
      "Hourly Activity Analysis:\n",
      "Top 5 hours by transaction volume:\n",
      "  Hour  0: 634,726.0 txns, ML rate: 0.0003\n",
      "  Hour 15: 194,871.0 txns, ML rate: 0.0013\n",
      "  Hour  6: 194,456.0 txns, ML rate: 0.0011\n",
      "  Hour  5: 193,900.0 txns, ML rate: 0.0010\n",
      "  Hour  1: 193,728.0 txns, ML rate: 0.0008\n",
      "\n",
      "Top 5 hours by ML rate:\n",
      "  Hour 12: ML rate: 0.0017, 336.0 ML txns\n",
      "  Hour 16: ML rate: 0.0016, 311.0 ML txns\n",
      "  Hour 11: ML rate: 0.0015, 295.0 ML txns\n",
      "  Hour 13: ML rate: 0.0015, 292.0 ML txns\n",
      "  Hour 14: ML rate: 0.0014, 279.0 ML txns\n",
      "\n",
      "Daily Activity Summary:\n",
      "  Avg transactions per day: 282130.3\n",
      "  Avg ML transactions per day: 287.6\n",
      "  Days with ML activity: 18\n",
      "  Peak day transactions: 1,114,921\n",
      "  Peak day ML transactions: 539\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing temporal patterns...\")\n",
    "\n",
    "transactions_df['hour'] = transactions_df['timestamp'].dt.hour\n",
    "transactions_df['day_of_week'] = transactions_df['timestamp'].dt.dayofweek\n",
    "transactions_df['date'] = transactions_df['timestamp'].dt.date\n",
    "\n",
    "hourly_activity = transactions_df.groupby('hour').agg({\n",
    "    'is_laundering': ['count', 'sum', 'mean'],\n",
    "    'amount_paid': ['sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "hourly_activity.columns = ['total_txns', 'ml_txns', 'ml_rate', 'total_amount', 'avg_amount']\n",
    "\n",
    "print(\"Hourly Activity Analysis:\")\n",
    "print(\"Top 5 hours by transaction volume:\")\n",
    "top_hours = hourly_activity.nlargest(5, 'total_txns')\n",
    "for hour, row in top_hours.iterrows():\n",
    "    print(f\"  Hour {hour:2d}: {row['total_txns']:,} txns, ML rate: {row['ml_rate']:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 hours by ML rate:\")\n",
    "top_ml_hours = hourly_activity.nlargest(5, 'ml_rate')\n",
    "for hour, row in top_ml_hours.iterrows():\n",
    "    print(f\"  Hour {hour:2d}: ML rate: {row['ml_rate']:.4f}, {row['ml_txns']} ML txns\")\n",
    "\n",
    "daily_activity = transactions_df.groupby('date').agg({\n",
    "    'is_laundering': ['count', 'sum', 'mean'],\n",
    "    'amount_paid': 'sum'\n",
    "}).round(4)\n",
    "\n",
    "daily_activity.columns = ['daily_txns', 'daily_ml_txns', 'daily_ml_rate', 'daily_amount']\n",
    "\n",
    "print(f\"\\nDaily Activity Summary:\")\n",
    "print(f\"  Avg transactions per day: {daily_activity['daily_txns'].mean():.1f}\")\n",
    "print(f\"  Avg ML transactions per day: {daily_activity['daily_ml_txns'].mean():.1f}\")\n",
    "print(f\"  Days with ML activity: {(daily_activity['daily_ml_txns'] > 0).sum()}\")\n",
    "print(f\"  Peak day transactions: {daily_activity['daily_txns'].max():,}\")\n",
    "print(f\"  Peak day ML transactions: {daily_activity['daily_ml_txns'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e783fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing payment formats and currencies...\n",
      "Payment Format Analysis:\n",
      "  ACH:\n",
      "    Transactions: 600,797.0 (11.83%)\n",
      "    ML transactions: 4483.0 (rate: 0.0075)\n",
      "    Avg amount: $9,497,986.77\n",
      "  Bitcoin:\n",
      "    Transactions: 146,091.0 (2.88%)\n",
      "    ML transactions: 56.0 (rate: 0.0004)\n",
      "    Avg amount: $30.83\n",
      "  Cash:\n",
      "    Transactions: 490,891.0 (9.67%)\n",
      "    ML transactions: 108.0 (rate: 0.0002)\n",
      "    Avg amount: $7,359,934.25\n",
      "  Cheque:\n",
      "    Transactions: 1,864,331.0 (36.71%)\n",
      "    ML transactions: 324.0 (rate: 0.0002)\n",
      "    Avg amount: $6,103,601.28\n",
      "  Credit Card:\n",
      "    Transactions: 1,323,324.0 (26.06%)\n",
      "    ML transactions: 206.0 (rate: 0.0002)\n",
      "    Avg amount: $86,783.37\n",
      "  Reinvestment:\n",
      "    Transactions: 481,056.0 (9.47%)\n",
      "    ML transactions: 0.0 (rate: 0.0000)\n",
      "    Avg amount: $2,595,009.06\n",
      "  Wire:\n",
      "    Transactions: 171,855.0 (3.38%)\n",
      "    ML transactions: 0.0 (rate: 0.0000)\n",
      "    Avg amount: $4,876,399.53\n",
      "\n",
      "Top 10 Currencies by Transaction Volume:\n",
      "  US Dollar: 1,895,172.0 txns, ML rate: 0.0010\n",
      "  Euro: 1,168,297.0 txns, ML rate: 0.0012\n",
      "  Swiss Franc: 234,860.0 txns, ML rate: 0.0008\n",
      "  Yuan: 213,752.0 txns, ML rate: 0.0009\n",
      "  Shekel: 192,184.0 txns, ML rate: 0.0005\n",
      "  Rupee: 190,202.0 txns, ML rate: 0.0009\n",
      "  UK Pound: 180,738.0 txns, ML rate: 0.0007\n",
      "  Yen: 155,209.0 txns, ML rate: 0.0010\n",
      "  Ruble: 155,178.0 txns, ML rate: 0.0009\n",
      "  Bitcoin: 146,066.0 txns, ML rate: 0.0004\n",
      "\n",
      "Currencies with high ML rates (>0.1%):\n",
      "  Saudi Riyal: ML rate 0.0042, 374.0 ML txns\n",
      "  Euro: ML rate 0.0012, 1372.0 ML txns\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing payment formats and currencies...\")\n",
    "\n",
    "payment_format_analysis = transactions_df.groupby('payment_format').agg({\n",
    "    'is_laundering': ['count', 'sum', 'mean'],\n",
    "    'amount_paid': ['sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "payment_format_analysis.columns = ['total_txns', 'ml_txns', 'ml_rate', 'total_amount', 'avg_amount']\n",
    "\n",
    "print(\"Payment Format Analysis:\")\n",
    "for format_type, row in payment_format_analysis.iterrows():\n",
    "    print(f\"  {format_type}:\")\n",
    "    print(f\"    Transactions: {row['total_txns']:,} ({row['total_txns']/total_transactions*100:.2f}%)\")\n",
    "    print(f\"    ML transactions: {row['ml_txns']} (rate: {row['ml_rate']:.4f})\")\n",
    "    print(f\"    Avg amount: ${row['avg_amount']:,.2f}\")\n",
    "\n",
    "currency_analysis = transactions_df.groupby('payment_currency').agg({\n",
    "    'is_laundering': ['count', 'sum', 'mean'],\n",
    "    'amount_paid': ['sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "currency_analysis.columns = ['total_txns', 'ml_txns', 'ml_rate', 'total_amount', 'avg_amount']\n",
    "currency_analysis = currency_analysis.sort_values('total_txns', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Currencies by Transaction Volume:\")\n",
    "for currency, row in currency_analysis.head(10).iterrows():\n",
    "    print(f\"  {currency}: {row['total_txns']:,} txns, ML rate: {row['ml_rate']:.4f}\")\n",
    "\n",
    "high_ml_currencies = currency_analysis[currency_analysis['ml_rate'] > 0.001].sort_values('ml_rate', ascending=False)\n",
    "print(f\"\\nCurrencies with high ML rates (>0.1%):\")\n",
    "for currency, row in high_ml_currencies.head(10).iterrows():\n",
    "    print(f\"  {currency}: ML rate {row['ml_rate']:.4f}, {row['ml_txns']} ML txns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f05d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing transaction amounts and distributions...\n",
      "Amount Distribution Analysis:\n",
      "\n",
      "Overall:\n",
      "  min: $0.00\n",
      "  max: $1,046,302,363,293.48\n",
      "  mean: $4,509,273.37\n",
      "  median: $1,414.54\n",
      "  std: $869,772,830.92\n",
      "\n",
      "Ml Transactions:\n",
      "  min: $0.00\n",
      "  max: $84,853,144,179.58\n",
      "  mean: $36,135,310.41\n",
      "  median: $8,667.21\n",
      "  std: $1,527,918,669.80\n",
      "\n",
      "Normal Transactions:\n",
      "  min: $0.00\n",
      "  max: $1,046,302,363,293.48\n",
      "  mean: $4,477,000.04\n",
      "  median: $1,410.99\n",
      "  std: $868,846,296.80\n",
      "\n",
      "Amount Range Analysis:\n",
      "  $0-100:\n",
      "    Transactions: 934,406 (18.40%)\n",
      "    ML transactions: 152 (rate: 0.0002)\n",
      "  $100-1K:\n",
      "    Transactions: 1,377,027 (27.12%)\n",
      "    ML transactions: 493 (rate: 0.0004)\n",
      "  $1K-10K:\n",
      "    Transactions: 1,396,292 (27.50%)\n",
      "    ML transactions: 2125 (rate: 0.0015)\n",
      "  $10K-100K:\n",
      "    Transactions: 793,993 (15.63%)\n",
      "    ML transactions: 1803 (rate: 0.0023)\n",
      "  $100K+:\n",
      "    Transactions: 576,627 (11.35%)\n",
      "    ML transactions: 604 (rate: 0.0010)\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing transaction amounts and distributions...\")\n",
    "\n",
    "amount_stats = {\n",
    "    'overall': {\n",
    "        'min': transactions_df['amount_paid'].min(),\n",
    "        'max': transactions_df['amount_paid'].max(),\n",
    "        'mean': transactions_df['amount_paid'].mean(),\n",
    "        'median': transactions_df['amount_paid'].median(),\n",
    "        'std': transactions_df['amount_paid'].std()\n",
    "    },\n",
    "    'ml_transactions': {\n",
    "        'min': ml_transactions['amount_paid'].min(),\n",
    "        'max': ml_transactions['amount_paid'].max(),\n",
    "        'mean': ml_transactions['amount_paid'].mean(),\n",
    "        'median': ml_transactions['amount_paid'].median(),\n",
    "        'std': ml_transactions['amount_paid'].std()\n",
    "    },\n",
    "    'normal_transactions': {\n",
    "        'min': normal_transactions['amount_paid'].min(),\n",
    "        'max': normal_transactions['amount_paid'].max(),\n",
    "        'mean': normal_transactions['amount_paid'].mean(),\n",
    "        'median': normal_transactions['amount_paid'].median(),\n",
    "        'std': normal_transactions['amount_paid'].std()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Amount Distribution Analysis:\")\n",
    "for category, stats in amount_stats.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"  {stat}: ${value:,.2f}\")\n",
    "\n",
    "amount_ranges = [(0, 100), (100, 1000), (1000, 10000), (10000, 100000), (100000, float('inf'))]\n",
    "range_labels = ['$0-100', '$100-1K', '$1K-10K', '$10K-100K', '$100K+']\n",
    "\n",
    "print(\"\\nAmount Range Analysis:\")\n",
    "for i, (min_amt, max_amt) in enumerate(amount_ranges):\n",
    "    if max_amt == float('inf'):\n",
    "        mask = transactions_df['amount_paid'] >= min_amt\n",
    "    else:\n",
    "        mask = (transactions_df['amount_paid'] >= min_amt) & (transactions_df['amount_paid'] < max_amt)\n",
    "    \n",
    "    range_txns = transactions_df[mask]\n",
    "    range_ml = range_txns[range_txns['is_laundering'] == 1]\n",
    "    \n",
    "    print(f\"  {range_labels[i]}:\")\n",
    "    print(f\"    Transactions: {len(range_txns):,} ({len(range_txns)/total_transactions*100:.2f}%)\")\n",
    "    print(f\"    ML transactions: {len(range_ml)} (rate: {len(range_ml)/len(range_txns):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b871b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing account behavior patterns...\n",
      "\n",
      "Origin Account Behavior Analysis:\n",
      "  Total accounts: 496,995\n",
      "  Accounts with ML activity: 3376\n",
      "  Avg transactions per account: 10.2\n",
      "  Max transactions per account: 168672\n",
      "  High activity accounts (top 5%): 25381\n",
      "  High activity ML rate: 0.0003\n",
      "  ML accounts avg transaction count: 150.8\n",
      "  ML accounts avg currencies used: 1.5\n",
      "  ML accounts avg payment formats: 2.9\n",
      "\n",
      "Destination Account Behavior Analysis:\n",
      "  Total accounts: 420,636\n",
      "  Accounts with ML activity: 3984\n",
      "  Avg transactions per account: 12.1\n",
      "  Max transactions per account: 1084\n",
      "  High activity accounts (top 5%): 22684\n",
      "  High activity ML rate: 0.0008\n",
      "  ML accounts avg transaction count: 17.6\n",
      "  ML accounts avg currencies used: 1.0\n",
      "  ML accounts avg payment formats: 3.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing account behavior patterns...\")\n",
    "\n",
    "def analyze_account_behavior(account_col, label):\n",
    "    account_behavior = transactions_df.groupby(account_col).agg({\n",
    "        'is_laundering': ['count', 'sum', 'mean'],\n",
    "        'amount_paid': ['sum', 'mean', 'std'],\n",
    "        'payment_format': lambda x: x.nunique(),\n",
    "        'payment_currency': lambda x: x.nunique(),\n",
    "        'timestamp': lambda x: (x.max() - x.min()).days\n",
    "    }).round(4)\n",
    "    \n",
    "    account_behavior.columns = ['txn_count', 'ml_count', 'ml_rate', 'total_amount', 'avg_amount', 'amount_std', 'payment_formats', 'currencies', 'active_days']\n",
    "    account_behavior['amount_std'] = account_behavior['amount_std'].fillna(0)\n",
    "    \n",
    "    print(f\"\\n{label} Account Behavior Analysis:\")\n",
    "    print(f\"  Total accounts: {len(account_behavior):,}\")\n",
    "    print(f\"  Accounts with ML activity: {(account_behavior['ml_count'] > 0).sum()}\")\n",
    "    print(f\"  Avg transactions per account: {account_behavior['txn_count'].mean():.1f}\")\n",
    "    print(f\"  Max transactions per account: {account_behavior['txn_count'].max()}\")\n",
    "    \n",
    "    high_activity_accounts = account_behavior[account_behavior['txn_count'] >= account_behavior['txn_count'].quantile(0.95)]\n",
    "    print(f\"  High activity accounts (top 5%): {len(high_activity_accounts)}\")\n",
    "    print(f\"  High activity ML rate: {high_activity_accounts['ml_rate'].mean():.4f}\")\n",
    "    \n",
    "    ml_accounts = account_behavior[account_behavior['ml_count'] > 0]\n",
    "    if len(ml_accounts) > 0:\n",
    "        print(f\"  ML accounts avg transaction count: {ml_accounts['txn_count'].mean():.1f}\")\n",
    "        print(f\"  ML accounts avg currencies used: {ml_accounts['currencies'].mean():.1f}\")\n",
    "        print(f\"  ML accounts avg payment formats: {ml_accounts['payment_formats'].mean():.1f}\")\n",
    "    \n",
    "    return account_behavior\n",
    "\n",
    "origin_behavior = analyze_account_behavior('account_origin', 'Origin')\n",
    "destination_behavior = analyze_account_behavior('account_destination', 'Destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f773ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing bank-level patterns...\n",
      "Bank Transaction Patterns:\n",
      "  Same-bank transactions: 691,332\n",
      "  Cross-bank transactions: 4,387,013\n",
      "  Same-bank ML rate: 0.0001\n",
      "  Cross-bank ML rate: 0.0012\n",
      "\n",
      "Banks involved in ML transactions: 1021\n",
      "Top 5 banks by ML transaction volume:\n",
      "  Bank 70: 633.0 ML txns, $395,491,904.00\n",
      "  Bank 12: 76.0 ML txns, $44,830,572.51\n",
      "  Bank 20: 67.0 ML txns, $1,934,260.92\n",
      "  Bank 119: 59.0 ML txns, $41,232,105.68\n",
      "  Bank 10: 51.0 ML txns, $1,074,345.13\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing bank-level patterns...\")\n",
    "\n",
    "bank_analysis = transactions_df.groupby(['from_bank', 'to_bank']).agg({\n",
    "    'is_laundering': ['count', 'sum', 'mean'],\n",
    "    'amount_paid': ['sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "bank_analysis.columns = ['txn_count', 'ml_count', 'ml_rate', 'total_amount', 'avg_amount']\n",
    "\n",
    "same_bank_txns = bank_analysis[bank_analysis.index.get_level_values(0) == bank_analysis.index.get_level_values(1)]\n",
    "cross_bank_txns = bank_analysis[bank_analysis.index.get_level_values(0) != bank_analysis.index.get_level_values(1)]\n",
    "\n",
    "print(\"Bank Transaction Patterns:\")\n",
    "print(f\"  Same-bank transactions: {same_bank_txns['txn_count'].sum():,}\")\n",
    "print(f\"  Cross-bank transactions: {cross_bank_txns['txn_count'].sum():,}\")\n",
    "print(f\"  Same-bank ML rate: {same_bank_txns['ml_count'].sum() / same_bank_txns['txn_count'].sum():.4f}\")\n",
    "print(f\"  Cross-bank ML rate: {cross_bank_txns['ml_count'].sum() / cross_bank_txns['txn_count'].sum():.4f}\")\n",
    "\n",
    "bank_ml_activity = transactions_df[transactions_df['is_laundering'] == 1].groupby('from_bank').agg({\n",
    "    'account_origin': 'nunique',\n",
    "    'account_destination': 'nunique', \n",
    "    'amount_paid': ['count', 'sum'],\n",
    "    'payment_currency': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "bank_ml_activity.columns = ['unique_origins', 'unique_destinations', 'ml_txn_count', 'ml_total_amount', 'currencies_used']\n",
    "\n",
    "print(f\"\\nBanks involved in ML transactions: {len(bank_ml_activity)}\")\n",
    "print(\"Top 5 banks by ML transaction volume:\")\n",
    "top_ml_banks = bank_ml_activity.nlargest(5, 'ml_txn_count')\n",
    "for bank, row in top_ml_banks.iterrows():\n",
    "    print(f\"  Bank {bank}: {row['ml_txn_count']} ML txns, ${row['ml_total_amount']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ca9ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing individual money laundering patterns...\n",
      "Detailed Pattern Analysis:\n",
      "\n",
      "FAN_OUT:\n",
      "  Instances: 48\n",
      "  Total transactions: 342\n",
      "  Avg transactions per instance: 7.1\n",
      "  Unique accounts involved: 359\n",
      "  Unique banks involved: 260\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 14\n",
      "  Total amount: $2,855,555,186.71\n",
      "  Avg amount per transaction: $8,349,576.57\n",
      "  Dominant currency: US Dollar\n",
      "\n",
      "CYCLE:\n",
      "  Instances: 54\n",
      "  Total transactions: 287\n",
      "  Avg transactions per instance: 5.3\n",
      "  Unique accounts involved: 271\n",
      "  Unique banks involved: 204\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 14\n",
      "  Total amount: $1,006,404,841.38\n",
      "  Avg amount per transaction: $3,506,637.08\n",
      "  Dominant currency: US Dollar\n",
      "\n",
      "GATHER_SCATTER:\n",
      "  Instances: 51\n",
      "  Total transactions: 716\n",
      "  Avg transactions per instance: 14.0\n",
      "  Unique accounts involved: 685\n",
      "  Unique banks involved: 422\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 14\n",
      "  Total amount: $1,210,493,226.40\n",
      "  Avg amount per transaction: $1,690,633.00\n",
      "  Dominant currency: US Dollar\n",
      "\n",
      "STACK:\n",
      "  Instances: 43\n",
      "  Total transactions: 466\n",
      "  Avg transactions per instance: 10.8\n",
      "  Unique accounts involved: 663\n",
      "  Unique banks involved: 408\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 15\n",
      "  Total amount: $1,130,230,001.29\n",
      "  Avg amount per transaction: $2,425,386.27\n",
      "  Dominant currency: US Dollar\n",
      "\n",
      "RANDOM:\n",
      "  Instances: 41\n",
      "  Total transactions: 191\n",
      "  Avg transactions per instance: 4.7\n",
      "  Unique accounts involved: 211\n",
      "  Unique banks involved: 162\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 13\n",
      "  Total amount: $566,749,825.44\n",
      "  Avg amount per transaction: $2,967,276.57\n",
      "  Dominant currency: US Dollar\n",
      "\n",
      "BIPARTITE:\n",
      "  Instances: 49\n",
      "  Total transactions: 263\n",
      "  Avg transactions per instance: 5.4\n",
      "  Unique accounts involved: 491\n",
      "  Unique banks involved: 311\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 13\n",
      "  Total amount: $356,110,762.54\n",
      "  Avg amount per transaction: $1,354,033.32\n",
      "  Dominant currency: Euro\n",
      "\n",
      "FAN_IN:\n",
      "  Instances: 40\n",
      "  Total transactions: 318\n",
      "  Avg transactions per instance: 8.0\n",
      "  Unique accounts involved: 338\n",
      "  Unique banks involved: 239\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 7\n",
      "  Total amount: $79,139,169.68\n",
      "  Avg amount per transaction: $248,865.31\n",
      "  Dominant currency: Euro\n",
      "\n",
      "SCATTER_GATHER:\n",
      "  Instances: 44\n",
      "  Total transactions: 626\n",
      "  Avg transactions per instance: 14.2\n",
      "  Unique accounts involved: 369\n",
      "  Unique banks involved: 276\n",
      "  Avg time span: 0.0 hours\n",
      "  Currencies used: 14\n",
      "  Total amount: $1,524,180,413.75\n",
      "  Avg amount per transaction: $2,434,792.99\n",
      "  Dominant currency: US Dollar\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing individual money laundering patterns...\")\n",
    "\n",
    "pattern_detailed_analysis = {}\n",
    "\n",
    "for pattern_type in patterns_df['pattern_type'].unique():\n",
    "    pattern_instances = patterns_df[patterns_df['pattern_type'] == pattern_type]\n",
    "    pattern_txns = pattern_transactions_df[pattern_transactions_df['pattern_type'] == pattern_type]\n",
    "    \n",
    "    unique_accounts = set(pattern_txns['account_origin'].unique()) | set(pattern_txns['account_destination'].unique())\n",
    "    unique_banks = set(pattern_txns['from_bank'].unique()) | set(pattern_txns['to_bank'].unique())\n",
    "    \n",
    "    time_spans = []\n",
    "    for _, instance in pattern_instances.iterrows():\n",
    "        if 'transactions' in instance:\n",
    "            instance_txns = pd.DataFrame(instance['transactions'])\n",
    "            instance_txns['timestamp'] = pd.to_datetime(instance_txns['timestamp'])\n",
    "            time_span = (instance_txns['timestamp'].max() - instance_txns['timestamp'].min()).total_seconds() / 3600\n",
    "            time_spans.append(time_span)\n",
    "    \n",
    "    amounts = pattern_txns['amount_paid'].astype(float)\n",
    "    \n",
    "    analysis = {\n",
    "        'instance_count': len(pattern_instances),\n",
    "        'total_transactions': len(pattern_txns),\n",
    "        'avg_txns_per_instance': len(pattern_txns) / len(pattern_instances) if len(pattern_instances) > 0 else 0,\n",
    "        'unique_accounts': len(unique_accounts),\n",
    "        'unique_banks': len(unique_banks),\n",
    "        'avg_time_span_hours': np.mean(time_spans) if time_spans else 0,\n",
    "        'currencies_used': pattern_txns['payment_currency'].nunique(),\n",
    "        'total_amount': amounts.sum(),\n",
    "        'avg_amount_per_txn': amounts.mean(),\n",
    "        'amount_std': amounts.std(),\n",
    "        'dominant_currency': pattern_txns['payment_currency'].mode()[0] if len(pattern_txns) > 0 else 'N/A'\n",
    "    }\n",
    "    \n",
    "    pattern_detailed_analysis[pattern_type] = analysis\n",
    "\n",
    "print(\"Detailed Pattern Analysis:\")\n",
    "for pattern_type, analysis in pattern_detailed_analysis.items():\n",
    "    print(f\"\\n{pattern_type}:\")\n",
    "    print(f\"  Instances: {analysis['instance_count']}\")\n",
    "    print(f\"  Total transactions: {analysis['total_transactions']}\")\n",
    "    print(f\"  Avg transactions per instance: {analysis['avg_txns_per_instance']:.1f}\")\n",
    "    print(f\"  Unique accounts involved: {analysis['unique_accounts']}\")\n",
    "    print(f\"  Unique banks involved: {analysis['unique_banks']}\")\n",
    "    print(f\"  Avg time span: {analysis['avg_time_span_hours']:.1f} hours\")\n",
    "    print(f\"  Currencies used: {analysis['currencies_used']}\")\n",
    "    print(f\"  Total amount: ${analysis['total_amount']:,.2f}\")\n",
    "    print(f\"  Avg amount per transaction: ${analysis['avg_amount_per_txn']:,.2f}\")\n",
    "    print(f\"  Dominant currency: {analysis['dominant_currency']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbe15707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing network characteristics for graph construction...\n",
      "Network Structure Analysis:\n",
      "  Accounts with outgoing connections: 496,995\n",
      "  Accounts with incoming connections: 420,636\n",
      "\n",
      "Out-degree statistics:\n",
      "  Mean: 2.04\n",
      "  Max: 14230\n",
      "  75th percentile: 2\n",
      "\n",
      "In-degree statistics:\n",
      "  Mean: 2.41\n",
      "  Max: 545\n",
      "  75th percentile: 3\n",
      "\n",
      "High connectivity accounts:\n",
      "  Accounts with 10+ outgoing connections: 6078\n",
      "  Accounts with 10+ incoming connections: 3326\n",
      "  High-degree accounts involved in ML: 498\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing network characteristics for graph construction...\")\n",
    "\n",
    "def calculate_network_metrics():\n",
    "    account_connections = defaultdict(set)\n",
    "    bank_connections = defaultdict(set)\n",
    "    \n",
    "    for _, txn in transactions_df.iterrows():\n",
    "        account_connections[txn['account_origin']].add(txn['account_destination'])\n",
    "        bank_connections[txn['from_bank']].add(txn['to_bank'])\n",
    "    \n",
    "    account_out_degrees = {acc: len(connections) for acc, connections in account_connections.items()}\n",
    "    account_in_degrees = defaultdict(int)\n",
    "    \n",
    "    for origin_acc, dest_accounts in account_connections.items():\n",
    "        for dest_acc in dest_accounts:\n",
    "            account_in_degrees[dest_acc] += 1\n",
    "    \n",
    "    return account_out_degrees, dict(account_in_degrees), account_connections, bank_connections\n",
    "\n",
    "account_out_degrees, account_in_degrees, account_connections, bank_connections = calculate_network_metrics()\n",
    "\n",
    "print(\"Network Structure Analysis:\")\n",
    "print(f\"  Accounts with outgoing connections: {len(account_out_degrees):,}\")\n",
    "print(f\"  Accounts with incoming connections: {len(account_in_degrees):,}\")\n",
    "\n",
    "out_degree_stats = pd.Series(list(account_out_degrees.values())).describe()\n",
    "in_degree_stats = pd.Series(list(account_in_degrees.values())).describe()\n",
    "\n",
    "print(f\"\\nOut-degree statistics:\")\n",
    "print(f\"  Mean: {out_degree_stats['mean']:.2f}\")\n",
    "print(f\"  Max: {out_degree_stats['max']:.0f}\")\n",
    "print(f\"  75th percentile: {out_degree_stats['75%']:.0f}\")\n",
    "\n",
    "print(f\"\\nIn-degree statistics:\")\n",
    "print(f\"  Mean: {in_degree_stats['mean']:.2f}\")\n",
    "print(f\"  Max: {in_degree_stats['max']:.0f}\")\n",
    "print(f\"  75th percentile: {in_degree_stats['75%']:.0f}\")\n",
    "\n",
    "high_out_degree_accounts = [acc for acc, degree in account_out_degrees.items() if degree >= 10]\n",
    "high_in_degree_accounts = [acc for acc, degree in account_in_degrees.items() if degree >= 10]\n",
    "\n",
    "print(f\"\\nHigh connectivity accounts:\")\n",
    "print(f\"  Accounts with 10+ outgoing connections: {len(high_out_degree_accounts)}\")\n",
    "print(f\"  Accounts with 10+ incoming connections: {len(high_in_degree_accounts)}\")\n",
    "\n",
    "ml_involvement = transactions_df[transactions_df['is_laundering'] == 1]\n",
    "ml_accounts = set(ml_involvement['account_origin'].unique()) | set(ml_involvement['account_destination'].unique())\n",
    "\n",
    "high_degree_ml_overlap = set(high_out_degree_accounts + high_in_degree_accounts) & ml_accounts\n",
    "print(f\"  High-degree accounts involved in ML: {len(high_degree_ml_overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b579ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing temporal clustering and velocity patterns...\n",
      "Temporal Clustering Analysis:\n",
      "  ML transaction time gaps (minutes):\n",
      "    Mean: 4.9\n",
      "    Median: 2.0\n",
      "    Min: 0.0\n",
      "  Rapid sequences (<1 hour apart): 5138 (99.3%)\n",
      "\n",
      "High-velocity ML accounts (top 10):\n",
      "  100428660: 243 txns, avg gap 59.2 min\n",
      "  1004286A8: 158 txns, avg gap 89.9 min\n",
      "  8040AE4F0: 16 txns, avg gap 195.9 min\n",
      "  80452D470: 16 txns, avg gap 293.7 min\n",
      "  100428810: 26 txns, avg gap 510.3 min\n",
      "  100428978: 29 txns, avg gap 511.2 min\n",
      "  1004286F0: 21 txns, avg gap 571.7 min\n",
      "  100428780: 21 txns, avg gap 628.8 min\n",
      "  100428738: 23 txns, avg gap 643.4 min\n",
      "  80266F880: 29 txns, avg gap 653.3 min\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing temporal clustering and velocity patterns...\")\n",
    "\n",
    "def analyze_temporal_clustering():\n",
    "    ml_txns = transactions_df[transactions_df['is_laundering'] == 1].copy()\n",
    "    ml_txns = ml_txns.sort_values('timestamp')\n",
    "    \n",
    "    time_gaps = []\n",
    "    for i in range(1, len(ml_txns)):\n",
    "        time_gap = (ml_txns.iloc[i]['timestamp'] - ml_txns.iloc[i-1]['timestamp']).total_seconds() / 60\n",
    "        time_gaps.append(time_gap)\n",
    "    \n",
    "    velocity_analysis = {}\n",
    "    \n",
    "    for account in ml_txns['account_origin'].value_counts().head(20).index:\n",
    "        account_txns = ml_txns[ml_txns['account_origin'] == account].sort_values('timestamp')\n",
    "        if len(account_txns) > 1:\n",
    "            account_gaps = []\n",
    "            for i in range(1, len(account_txns)):\n",
    "                gap = (account_txns.iloc[i]['timestamp'] - account_txns.iloc[i-1]['timestamp']).total_seconds() / 60\n",
    "                account_gaps.append(gap)\n",
    "            \n",
    "            velocity_analysis[account] = {\n",
    "                'txn_count': len(account_txns),\n",
    "                'avg_gap_minutes': np.mean(account_gaps),\n",
    "                'min_gap_minutes': min(account_gaps),\n",
    "                'total_span_hours': (account_txns['timestamp'].max() - account_txns['timestamp'].min()).total_seconds() / 3600\n",
    "            }\n",
    "    \n",
    "    return time_gaps, velocity_analysis\n",
    "\n",
    "time_gaps, velocity_analysis = analyze_temporal_clustering()\n",
    "\n",
    "print(\"Temporal Clustering Analysis:\")\n",
    "if time_gaps:\n",
    "    gap_stats = pd.Series(time_gaps).describe()\n",
    "    print(f\"  ML transaction time gaps (minutes):\")\n",
    "    print(f\"    Mean: {gap_stats['mean']:.1f}\")\n",
    "    print(f\"    Median: {gap_stats['50%']:.1f}\")\n",
    "    print(f\"    Min: {gap_stats['min']:.1f}\")\n",
    "    \n",
    "    rapid_sequences = [gap for gap in time_gaps if gap < 60]\n",
    "    print(f\"  Rapid sequences (<1 hour apart): {len(rapid_sequences)} ({len(rapid_sequences)/len(time_gaps)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nHigh-velocity ML accounts (top 10):\")\n",
    "sorted_velocity = sorted(velocity_analysis.items(), key=lambda x: x[1]['avg_gap_minutes'])\n",
    "for account, stats in sorted_velocity[:10]:\n",
    "    print(f\"  {account}: {stats['txn_count']} txns, avg gap {stats['avg_gap_minutes']:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a86246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating insights summary for graph construction...\n",
      "\n",
      "KEY INSIGHTS FOR GRAPH CONSTRUCTION:\n",
      "==================================================\n",
      "\n",
      "Temporal Insights:\n",
      "  Peak activity hours: [0, 15, 6]\n",
      "  ML concentrated hours: [12, 16, 11]\n",
      "  Rapid ML sequences: 99.3%\n",
      "\n",
      "Network Structure Insights:\n",
      "  High-degree accounts: 6078 out, 3326 in\n",
      "  ML-involved high-degree accounts: 498\n",
      "  Same-bank transactions: 13.6%\n",
      "\n",
      "Pattern Insights:\n",
      "  Most common pattern: CYCLE\n",
      "  Most complex pattern: SCATTER_GATHER\n",
      "  Longest timespan pattern: FAN_OUT\n",
      "\n",
      "Currency & Amount Insights:\n",
      "  Total currencies: 15\n",
      "  ML avg/normal avg ratio: 8.07\n",
      "  Large amount ML rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating insights summary for graph construction...\")\n",
    "\n",
    "insights_summary = {\n",
    "    'temporal_insights': {\n",
    "        'peak_hours': list(hourly_activity.nlargest(3, 'total_txns').index),\n",
    "        'ml_concentrated_hours': list(hourly_activity.nlargest(3, 'ml_rate').index),\n",
    "        'rapid_ml_sequences_percentage': len(rapid_sequences)/len(time_gaps)*100 if time_gaps else 0,\n",
    "        'avg_ml_time_gap_minutes': np.mean(time_gaps) if time_gaps else 0\n",
    "    },\n",
    "    'network_insights': {\n",
    "        'high_connectivity_threshold': 10,\n",
    "        'high_out_degree_accounts': len(high_out_degree_accounts),\n",
    "        'high_in_degree_accounts': len(high_in_degree_accounts),\n",
    "        'ml_high_degree_overlap': len(high_degree_ml_overlap),\n",
    "        'same_bank_transaction_percentage': same_bank_txns['txn_count'].sum() / total_transactions * 100,\n",
    "        'cross_bank_ml_rate': cross_bank_txns['ml_count'].sum() / cross_bank_txns['txn_count'].sum()\n",
    "    },\n",
    "    'pattern_insights': {\n",
    "        'most_common_pattern': patterns_df['pattern_type'].value_counts().index[0],\n",
    "        'most_complex_pattern': max(pattern_detailed_analysis.items(), key=lambda x: x[1]['avg_txns_per_instance'])[0],\n",
    "        'longest_timespan_pattern': max(pattern_detailed_analysis.items(), key=lambda x: x[1]['avg_time_span_hours'])[0],\n",
    "        'highest_value_pattern': max(pattern_detailed_analysis.items(), key=lambda x: x[1]['total_amount'])[0]\n",
    "    },\n",
    "    'currency_insights': {\n",
    "        'total_currencies': transactions_df['payment_currency'].nunique(),\n",
    "        'ml_currencies': ml_transactions['payment_currency'].nunique(),\n",
    "        'dominant_ml_currency': ml_transactions['payment_currency'].mode()[0],\n",
    "        'high_ml_rate_currencies': len(high_ml_currencies)\n",
    "    },\n",
    "    'amount_insights': {\n",
    "        'ml_vs_normal_avg_ratio': amount_stats['ml_transactions']['mean'] / amount_stats['normal_transactions']['mean'],\n",
    "        'large_amount_ml_rate': len(transactions_df[(transactions_df['amount_paid'] > 100000) & (transactions_df['is_laundering'] == 1)]) / len(transactions_df[transactions_df['amount_paid'] > 100000]),\n",
    "        'small_amount_ml_rate': len(transactions_df[(transactions_df['amount_paid'] < 1000) & (transactions_df['is_laundering'] == 1)]) / len(transactions_df[transactions_df['amount_paid'] < 1000])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nKEY INSIGHTS FOR GRAPH CONSTRUCTION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nTemporal Insights:\")\n",
    "print(f\"  Peak activity hours: {insights_summary['temporal_insights']['peak_hours']}\")\n",
    "print(f\"  ML concentrated hours: {insights_summary['temporal_insights']['ml_concentrated_hours']}\")\n",
    "print(f\"  Rapid ML sequences: {insights_summary['temporal_insights']['rapid_ml_sequences_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\nNetwork Structure Insights:\")\n",
    "print(f\"  High-degree accounts: {insights_summary['network_insights']['high_out_degree_accounts']} out, {insights_summary['network_insights']['high_in_degree_accounts']} in\")\n",
    "print(f\"  ML-involved high-degree accounts: {insights_summary['network_insights']['ml_high_degree_overlap']}\")\n",
    "print(f\"  Same-bank transactions: {insights_summary['network_insights']['same_bank_transaction_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\nPattern Insights:\")\n",
    "print(f\"  Most common pattern: {insights_summary['pattern_insights']['most_common_pattern']}\")\n",
    "print(f\"  Most complex pattern: {insights_summary['pattern_insights']['most_complex_pattern']}\")\n",
    "print(f\"  Longest timespan pattern: {insights_summary['pattern_insights']['longest_timespan_pattern']}\")\n",
    "\n",
    "print(\"\\nCurrency & Amount Insights:\")\n",
    "print(f\"  Total currencies: {insights_summary['currency_insights']['total_currencies']}\")\n",
    "print(f\"  ML avg/normal avg ratio: {insights_summary['amount_insights']['ml_vs_normal_avg_ratio']:.2f}\")\n",
    "print(f\"  Large amount ML rate: {insights_summary['amount_insights']['large_amount_ml_rate']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60b3d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving analysis results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving analysis results...\")\n",
    "\n",
    "origin_behavior.to_csv('../data/origin_account_behavior.csv')\n",
    "destination_behavior.to_csv('../data/destination_account_behavior.csv')\n",
    "bank_analysis.to_csv('../data/bank_transaction_analysis.csv')\n",
    "hourly_activity.to_csv('../data/hourly_activity_analysis.csv')\n",
    "daily_activity.to_csv('../data/daily_activity_analysis.csv')\n",
    "\n",
    "import pickle\n",
    "with open('../data/pattern_detailed_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(pattern_detailed_analysis, f)\n",
    "\n",
    "with open('../data/insights_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(insights_summary, f)\n",
    "\n",
    "with open('../data/network_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'account_out_degrees': account_out_degrees,\n",
    "        'account_in_degrees': account_in_degrees,\n",
    "        'account_connections': dict(account_connections),\n",
    "        'bank_connections': dict(bank_connections)\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
