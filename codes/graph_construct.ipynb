{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ac07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae902ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAMLGraphConstructor:\n",
    "    def __init__(self, batch_size=10000, checkpoint_dir='checkpoints'):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.graphs = {}\n",
    "        self.annotations = {}\n",
    "        self.node_mappings = {}\n",
    "        self.preprocessors = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        self.creation_timestamp = datetime.now().isoformat()\n",
    "        self.version = \"1.0_optimized\"\n",
    "        \n",
    "    def save_checkpoint(self, stage, data, step=None):\n",
    "        checkpoint_name = f\"{stage}_{step}\" if step else stage\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"{checkpoint_name}.pkl\")\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'stage': stage,\n",
    "            'step': step,\n",
    "            'data': data,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        \n",
    "        print(f\"Checkpoint saved: {checkpoint_name} ({checkpoint_data['memory_usage']:.1f}MB)\")\n",
    "        \n",
    "    def load_checkpoint(self, stage, step=None):\n",
    "        checkpoint_name = f\"{stage}_{step}\" if step else stage\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"{checkpoint_name}.pkl\")\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "            print(f\"Loaded checkpoint: {checkpoint_name}\")\n",
    "            return checkpoint_data['data']\n",
    "        return None\n",
    "        \n",
    "    def check_resume_point(self):\n",
    "        checkpoints = []\n",
    "        if os.path.exists(self.checkpoint_dir):\n",
    "            for file in os.listdir(self.checkpoint_dir):\n",
    "                if file.endswith('.pkl'):\n",
    "                    checkpoints.append(file.replace('.pkl', ''))\n",
    "        \n",
    "        resume_order = [\n",
    "            'data_loaded', 'node_mappings', 'account_features_batched',\n",
    "            'transaction_flow_graph', 'temporal_proximity_graph',\n",
    "            'account_behavior_graph', 'multimodal_integration_graph',\n",
    "            'ground_truth_pattern_graph', 'ego_networks'\n",
    "        ]\n",
    "        \n",
    "        for checkpoint in reversed(resume_order):\n",
    "            if checkpoint in checkpoints:\n",
    "                return checkpoint\n",
    "        return None\n",
    "\n",
    "    def monitor_memory(self, stage):\n",
    "        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        print(f\"[{stage}] Memory usage: {memory_mb:.1f}MB\")\n",
    "        \n",
    "        if memory_mb > 14000:\n",
    "            print(\"WARNING: High memory usage, forcing garbage collection\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbae0342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    constructor = OptimizedAMLGraphConstructor()\n",
    "    \n",
    "    resume_point = constructor.check_resume_point()\n",
    "    if resume_point:\n",
    "        print(f\"Resuming from checkpoint: {resume_point}\")\n",
    "        return constructor, resume_point\n",
    "    \n",
    "    print(\"Loading data from scratch...\")\n",
    "    \n",
    "    data = constructor.load_checkpoint('data_loaded')\n",
    "    if data is None:\n",
    "        transactions_df = pd.read_csv('../data/processed_transactions.csv')\n",
    "        accounts_df = pd.read_csv('../data/processed_accounts.csv')\n",
    "        patterns_df = pd.read_csv('../data/processed_patterns.csv')\n",
    "        pattern_transactions_df = pd.read_csv('../data/processed_pattern_transactions.csv')\n",
    "        \n",
    "        transactions_df['timestamp'] = pd.to_datetime(transactions_df['timestamp'])\n",
    "        pattern_transactions_df['timestamp'] = pd.to_datetime(pattern_transactions_df['timestamp'])\n",
    "        \n",
    "        transactions_df = transactions_df.astype({\n",
    "            'amount_paid': 'float32',\n",
    "            'amount_received': 'float32',\n",
    "            'is_laundering': 'int8'\n",
    "        })\n",
    "        \n",
    "        transactions_df = transactions_df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        data = {\n",
    "            'transactions_df': transactions_df,\n",
    "            'accounts_df': accounts_df,\n",
    "            'patterns_df': patterns_df,\n",
    "            'pattern_transactions_df': pattern_transactions_df\n",
    "        }\n",
    "        \n",
    "        constructor.save_checkpoint('data_loaded', data)\n",
    "    \n",
    "    constructor.monitor_memory('data_loaded')\n",
    "    return constructor, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663f76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_node_mappings(constructor, data):\n",
    "    node_mappings = constructor.load_checkpoint('node_mappings')\n",
    "    if node_mappings is not None:\n",
    "        constructor.node_mappings = node_mappings\n",
    "        return\n",
    "    \n",
    "    print(\"Creating optimized node mappings...\")\n",
    "    transactions_df = data['transactions_df']\n",
    "    \n",
    "    unique_accounts = set(transactions_df['account_origin'].unique()) | set(transactions_df['account_destination'].unique())\n",
    "    unique_banks = set(transactions_df['from_bank'].unique()) | set(transactions_df['to_bank'].unique())\n",
    "    \n",
    "    node_mappings = {\n",
    "        'accounts': {acc: idx for idx, acc in enumerate(sorted(unique_accounts))},\n",
    "        'banks': {bank: idx for idx, bank in enumerate(sorted(unique_banks))},\n",
    "        'transactions': {idx: idx for idx in range(len(transactions_df))}\n",
    "    }\n",
    "    \n",
    "    constructor.node_mappings = node_mappings\n",
    "    constructor.save_checkpoint('node_mappings', node_mappings)\n",
    "    constructor.monitor_memory('node_mappings')\n",
    "\n",
    "def engineer_account_features_batched(constructor, data):\n",
    "    account_features = constructor.load_checkpoint('account_features_batched')\n",
    "    if account_features is not None:\n",
    "        return account_features\n",
    "\n",
    "    print(\"Engineering account features with batched processing...\")\n",
    "    transactions_df = data['transactions_df']\n",
    "    node_mappings = constructor.node_mappings\n",
    "\n",
    "    accounts = list(node_mappings['accounts'].keys())\n",
    "    batch_size = constructor.batch_size\n",
    "    total_batches = (len(accounts) + batch_size - 1) // batch_size\n",
    "\n",
    "    account_features = {}\n",
    "\n",
    "    transactions_indexed = transactions_df.set_index(['account_origin', 'account_destination'])\n",
    "\n",
    "    outgoing_stats = transactions_df.groupby('account_origin').agg({\n",
    "        'amount_paid': ['count', 'sum', 'mean', 'std'],\n",
    "        'account_destination': 'nunique',\n",
    "        'payment_currency': 'nunique',\n",
    "        'payment_format': 'nunique',\n",
    "        'is_laundering': 'sum',\n",
    "        'timestamp': ['min', 'max']\n",
    "    }).fillna(0)\n",
    "\n",
    "    incoming_stats = transactions_df.groupby('account_destination').agg({\n",
    "        'amount_paid': ['count', 'sum', 'mean'],\n",
    "        'account_origin': 'nunique',\n",
    "        'is_laundering': 'sum'\n",
    "    }).fillna(0)\n",
    "\n",
    "    outgoing_stats.columns = ['_'.join(col).strip() for col in outgoing_stats.columns]\n",
    "    incoming_stats.columns = ['_'.join(col).strip() for col in incoming_stats.columns]\n",
    "\n",
    "    feature_names = [\n",
    "        'total_outgoing', 'total_incoming', 'total_transactions',\n",
    "        'avg_outgoing_amount', 'avg_incoming_amount',\n",
    "        'total_outgoing_volume', 'total_incoming_volume',\n",
    "        'unique_recipients', 'unique_senders',\n",
    "        'currency_diversity', 'payment_format_diversity',\n",
    "        'transaction_velocity', 'fan_out_degree', 'fan_in_degree',\n",
    "        'ml_rate', 'round_amount_ratio', 'outgoing_time_span'\n",
    "    ]\n",
    "\n",
    "    for batch_idx in tqdm(range(total_batches), desc=\"Processing account batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(accounts))\n",
    "        batch_accounts = accounts[start_idx:end_idx]\n",
    "        \n",
    "        for account in batch_accounts:\n",
    "            out_stats = outgoing_stats.loc[account] if account in outgoing_stats.index else pd.Series(0, index=outgoing_stats.columns)\n",
    "            in_stats = incoming_stats.loc[account] if account in incoming_stats.index else pd.Series(0, index=incoming_stats.columns)\n",
    "            \n",
    "            total_outgoing = out_stats.get('amount_paid_count', 0)\n",
    "            total_incoming = in_stats.get('amount_paid_count', 0)\n",
    "            \n",
    "            time_span_seconds = 0\n",
    "            if 'timestamp_max' in out_stats and 'timestamp_min' in out_stats:\n",
    "                if pd.notna(out_stats['timestamp_max']) and pd.notna(out_stats['timestamp_min']):\n",
    "                    time_span_seconds = (pd.to_datetime(out_stats['timestamp_max']) - pd.to_datetime(out_stats['timestamp_min'])).total_seconds()\n",
    "            \n",
    "            outgoing_time_span = time_span_seconds / 3600 if time_span_seconds > 0 else 0\n",
    "            transaction_velocity = total_outgoing / max(outgoing_time_span, 1)\n",
    "            \n",
    "            ml_involvement = out_stats.get('is_laundering_sum', 0) + in_stats.get('is_laundering_sum', 0)\n",
    "            ml_rate = ml_involvement / max(total_outgoing + total_incoming, 1)\n",
    "            \n",
    "            round_amount_ratio = 0\n",
    "            if total_outgoing > 0:\n",
    "                outgoing_amounts = transactions_df[transactions_df['account_origin'] == account]['amount_paid']\n",
    "                if len(outgoing_amounts) > 0:\n",
    "                    round_amounts = (outgoing_amounts % 100 == 0).sum()\n",
    "                    round_amount_ratio = round_amounts / len(outgoing_amounts)\n",
    "            \n",
    "            features = [\n",
    "                float(total_outgoing),\n",
    "                float(total_incoming),\n",
    "                float(total_outgoing + total_incoming),\n",
    "                float(out_stats.get('amount_paid_mean', 0)),\n",
    "                float(in_stats.get('amount_paid_mean', 0)),\n",
    "                float(out_stats.get('amount_paid_sum', 0)),\n",
    "                float(in_stats.get('amount_paid_sum', 0)),\n",
    "                float(out_stats.get('account_destination_nunique', 0)),\n",
    "                float(in_stats.get('account_origin_nunique', 0)),\n",
    "                float(out_stats.get('payment_currency_nunique', 0)),\n",
    "                float(out_stats.get('payment_format_nunique', 0)),\n",
    "                float(transaction_velocity),\n",
    "                float(out_stats.get('account_destination_nunique', 0)),\n",
    "                float(in_stats.get('account_origin_nunique', 0)),\n",
    "                float(ml_rate),\n",
    "                float(round_amount_ratio),\n",
    "                float(outgoing_time_span)\n",
    "            ]\n",
    "            \n",
    "            account_features[account] = features\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            constructor.monitor_memory(f'feature_batch_{batch_idx}')\n",
    "            gc.collect()\n",
    "\n",
    "    result = {\n",
    "        'account_features': account_features,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "    constructor.save_checkpoint('account_features_batched', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64909dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_transaction_flow_graph(constructor, data, account_features_data):\n",
    "    graph_data = constructor.load_checkpoint('transaction_flow_graph')\n",
    "    if graph_data is not None:\n",
    "        constructor.graphs['transaction_flow'] = graph_data['graph']\n",
    "        constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "        return\n",
    "    \n",
    "    print(\"Building optimized transaction flow graph...\")\n",
    "    transactions_df = data['transactions_df']\n",
    "    node_mappings = constructor.node_mappings\n",
    "    account_features = account_features_data['account_features']\n",
    "    feature_names = account_features_data['feature_names']\n",
    "    \n",
    "    payment_encoder = LabelEncoder()\n",
    "    currency_encoder = LabelEncoder()\n",
    "    \n",
    "    payment_encoder.fit(transactions_df['payment_format'].unique())\n",
    "    currency_encoder.fit(transactions_df['payment_currency'].unique())\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    batch_size = 100000\n",
    "    total_batches = (len(transactions_df) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in tqdm(range(total_batches), desc=\"Processing transaction batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(transactions_df))\n",
    "        batch_df = transactions_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for _, txn in batch_df.iterrows():\n",
    "            if txn['account_origin'] in node_mappings['accounts'] and txn['account_destination'] in node_mappings['accounts']:\n",
    "                source_idx = node_mappings['accounts'][txn['account_origin']]\n",
    "                target_idx = node_mappings['accounts'][txn['account_destination']]\n",
    "                \n",
    "                edge_list.append([source_idx, target_idx])\n",
    "                \n",
    "                edge_attr = [\n",
    "                    float(txn['amount_paid']),\n",
    "                    float(txn['amount_received']),\n",
    "                    float(payment_encoder.transform([txn['payment_format']])[0]),\n",
    "                    float(currency_encoder.transform([txn['payment_currency']])[0]),\n",
    "                    float(txn['timestamp'].hour),\n",
    "                    float(txn['timestamp'].dayofweek),\n",
    "                    float(txn['is_laundering'])\n",
    "                ]\n",
    "                edge_attributes.append(edge_attr)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            constructor.monitor_memory(f'tx_flow_batch_{batch_idx}')\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float32)\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    node_features = []\n",
    "    node_labels = []\n",
    "    \n",
    "    ml_accounts = set(transactions_df[transactions_df['is_laundering'] == 1]['account_origin']) | set(transactions_df[transactions_df['is_laundering'] == 1]['account_destination'])\n",
    "    \n",
    "    for account in account_list:\n",
    "        features = account_features[account]\n",
    "        node_features.append(features)\n",
    "        node_labels.append(1 if account in ml_accounts else 0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    node_features_scaled = scaler.fit_transform(node_features)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float32)\n",
    "    y = torch.tensor(node_labels, dtype=torch.long)\n",
    "    \n",
    "    transaction_flow_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(account_list)\n",
    "    )\n",
    "    \n",
    "    graph_data = {\n",
    "        'graph': transaction_flow_graph,\n",
    "        'preprocessors': {\n",
    "            'feature_scaler': scaler,\n",
    "            'payment_encoder': payment_encoder,\n",
    "            'currency_encoder': currency_encoder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['transaction_flow'] = transaction_flow_graph\n",
    "    constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "    constructor.save_checkpoint('transaction_flow_graph', graph_data)\n",
    "    constructor.monitor_memory('transaction_flow_complete')\n",
    "\n",
    "def build_sampled_temporal_proximity_graph(constructor, data):\n",
    "    graph_data = constructor.load_checkpoint('temporal_proximity_graph')\n",
    "    if graph_data is not None:\n",
    "        constructor.graphs['temporal_proximity'] = graph_data['graph']\n",
    "        constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "        return\n",
    "    \n",
    "    print(\"Building sampled temporal proximity graph...\")\n",
    "    transactions_df = data['transactions_df']\n",
    "    \n",
    "    sample_size = min(500000, len(transactions_df))\n",
    "    sampled_indices = np.random.choice(len(transactions_df), sample_size, replace=False)\n",
    "    sampled_transactions = transactions_df.iloc[sorted(sampled_indices)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Sampled {sample_size:,} transactions from {len(transactions_df):,}\")\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    time_window_hours = 24\n",
    "    max_connections_per_transaction = 50\n",
    "    \n",
    "    payment_encoder = constructor.preprocessors['payment_encoder']\n",
    "    currency_encoder = constructor.preprocessors['currency_encoder']\n",
    "    \n",
    "    for i in tqdm(range(len(sampled_transactions)), desc=\"Building temporal edges\"):\n",
    "        current_txn = sampled_transactions.iloc[i]\n",
    "        current_time = current_txn['timestamp']\n",
    "        \n",
    "        window_start = i + 1\n",
    "        window_end = min(i + max_connections_per_transaction, len(sampled_transactions))\n",
    "        \n",
    "        connections_made = 0\n",
    "        \n",
    "        for j in range(window_start, window_end):\n",
    "            if connections_made >= max_connections_per_transaction:\n",
    "                break\n",
    "                \n",
    "            next_txn = sampled_transactions.iloc[j]\n",
    "            next_time = next_txn['timestamp']\n",
    "            \n",
    "            time_diff = (next_time - current_time).total_seconds() / 3600\n",
    "            \n",
    "            if time_diff > time_window_hours:\n",
    "                break\n",
    "            \n",
    "            account_overlap = 0\n",
    "            if (current_txn['account_origin'] == next_txn['account_origin'] or\n",
    "                current_txn['account_destination'] == next_txn['account_destination'] or\n",
    "                current_txn['account_origin'] == next_txn['account_destination'] or\n",
    "                current_txn['account_destination'] == next_txn['account_origin']):\n",
    "                account_overlap = 1\n",
    "            \n",
    "            amount_similarity = 1 - min(abs(current_txn['amount_paid'] - next_txn['amount_paid']) / max(current_txn['amount_paid'] + next_txn['amount_paid'], 1), 1)\n",
    "            \n",
    "            edge_list.append([i, j])\n",
    "            edge_attributes.append([\n",
    "                float(time_diff),\n",
    "                float(account_overlap),\n",
    "                float(amount_similarity),\n",
    "                float(current_txn['is_laundering']),\n",
    "                float(next_txn['is_laundering'])\n",
    "            ])\n",
    "            \n",
    "            connections_made += 1\n",
    "        \n",
    "        if i % 50000 == 0:\n",
    "            constructor.monitor_memory(f'temporal_edge_{i}')\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        print(\"No temporal edges found, creating minimal graph\")\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, 5), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attributes, dtype=torch.float32)\n",
    "    \n",
    "    transaction_features = []\n",
    "    transaction_labels = []\n",
    "    \n",
    "    for _, txn in sampled_transactions.iterrows():\n",
    "        features = [\n",
    "            float(np.log1p(txn['amount_paid'])),\n",
    "            float(txn['timestamp'].hour),\n",
    "            float(txn['timestamp'].dayofweek),\n",
    "            float(payment_encoder.transform([txn['payment_format']])[0]),\n",
    "            float(currency_encoder.transform([txn['payment_currency']])[0])\n",
    "        ]\n",
    "        transaction_features.append(features)\n",
    "        transaction_labels.append(int(txn['is_laundering']))\n",
    "    \n",
    "    scaler_temporal = StandardScaler()\n",
    "    transaction_features_scaled = scaler_temporal.fit_transform(transaction_features)\n",
    "    \n",
    "    x = torch.tensor(transaction_features_scaled, dtype=torch.float32)\n",
    "    y = torch.tensor(transaction_labels, dtype=torch.long)\n",
    "    \n",
    "    temporal_proximity_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(sampled_transactions)\n",
    "    )\n",
    "    \n",
    "    graph_data = {\n",
    "        'graph': temporal_proximity_graph,\n",
    "        'preprocessors': {\n",
    "            'temporal_scaler': scaler_temporal\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['temporal_proximity'] = temporal_proximity_graph\n",
    "    constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "    constructor.save_checkpoint('temporal_proximity_graph', graph_data)\n",
    "    constructor.monitor_memory('temporal_proximity_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975208f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficient_account_behavior_graph(constructor, account_features_data):\n",
    "    graph_data = constructor.load_checkpoint('account_behavior_graph')\n",
    "    if graph_data is not None:\n",
    "        constructor.graphs['account_behavior'] = graph_data['graph']\n",
    "        constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "        return\n",
    "    \n",
    "    print(\"Building efficient account behavior graph...\")\n",
    "    account_features = account_features_data['account_features']\n",
    "    node_mappings = constructor.node_mappings\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    \n",
    "    feature_matrix = np.array([account_features[acc] for acc in account_list], dtype=np.float32)\n",
    "    \n",
    "    scaler_behavior = StandardScaler()\n",
    "    feature_matrix_scaled = scaler_behavior.fit_transform(feature_matrix)\n",
    "    \n",
    "    batch_size = 5000\n",
    "    similarity_threshold = 0.7\n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    total_batches = (len(account_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(total_batches), desc=\"Computing similarities\"):\n",
    "        start_i = i * batch_size\n",
    "        end_i = min((i + 1) * batch_size, len(account_list))\n",
    "        \n",
    "        for j in range(i, total_batches):\n",
    "            start_j = j * batch_size\n",
    "            end_j = min((j + 1) * batch_size, len(account_list))\n",
    "            \n",
    "            batch_similarities = cosine_similarity(\n",
    "                feature_matrix_scaled[start_i:end_i],\n",
    "                feature_matrix_scaled[start_j:end_j]\n",
    "            )\n",
    "            \n",
    "            for local_i, global_i in enumerate(range(start_i, end_i)):\n",
    "                start_local_j = local_i if i == j else 0\n",
    "                for local_j, global_j in enumerate(range(start_j, end_j)):\n",
    "                    if global_j <= global_i:\n",
    "                        continue\n",
    "                    \n",
    "                    similarity = batch_similarities[local_i, local_j]\n",
    "                    if similarity > similarity_threshold:\n",
    "                        edge_list.extend([[global_i, global_j], [global_j, global_i]])\n",
    "                        edge_attributes.extend([[similarity], [similarity]])\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            constructor.monitor_memory(f'behavior_similarity_{i}')\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        print(f\"No edges with threshold {similarity_threshold}, lowering to 0.5\")\n",
    "        similarity_threshold = 0.5\n",
    "        \n",
    "        for i in tqdm(range(0, min(10000, len(account_list))), desc=\"Computing with lower threshold\"):\n",
    "            for j in range(i + 1, min(10000, len(account_list))):\n",
    "                similarity = cosine_similarity([feature_matrix_scaled[i]], [feature_matrix_scaled[j]])[0, 0]\n",
    "                if similarity > similarity_threshold:\n",
    "                    edge_list.extend([[i, j], [j, i]])\n",
    "                    edge_attributes.extend([[similarity], [similarity]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float32) if edge_attributes else torch.empty((0, 1), dtype=torch.float32)\n",
    "    \n",
    "    x = torch.tensor(feature_matrix_scaled, dtype=torch.float32)\n",
    "    \n",
    "    ml_accounts = set()\n",
    "    for graph_name, graph in constructor.graphs.items():\n",
    "        if hasattr(graph, 'y') and graph_name == 'transaction_flow':\n",
    "            for i, label in enumerate(graph.y):\n",
    "                if label == 1:\n",
    "                    ml_accounts.add(account_list[i])\n",
    "    \n",
    "    y = torch.tensor([1 if acc in ml_accounts else 0 for acc in account_list], dtype=torch.long)\n",
    "    \n",
    "    account_behavior_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(account_list)\n",
    "    )\n",
    "    \n",
    "    graph_data = {\n",
    "        'graph': account_behavior_graph,\n",
    "        'preprocessors': {\n",
    "            'behavior_scaler': scaler_behavior\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['account_behavior'] = account_behavior_graph\n",
    "    constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "    constructor.save_checkpoint('account_behavior_graph', graph_data)\n",
    "    constructor.monitor_memory('account_behavior_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15dbc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_multimodal_graph(constructor, data, account_features_data):\n",
    "    graph_data = constructor.load_checkpoint('multimodal_integration_graph')\n",
    "    if graph_data is not None:\n",
    "        constructor.graphs['multimodal_integration'] = graph_data['graph']\n",
    "        constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "        return\n",
    "    \n",
    "    print(\"Building optimized multimodal integration graph...\")\n",
    "    transactions_df = data['transactions_df']\n",
    "    node_mappings = constructor.node_mappings\n",
    "    account_features = account_features_data['account_features']\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    bank_list = sorted(node_mappings['banks'].keys())\n",
    "    \n",
    "    node_features_list = []\n",
    "    node_types = []\n",
    "    account_node_mapping = {}\n",
    "    bank_node_mapping = {}\n",
    "    node_idx = 0\n",
    "    \n",
    "    for account in account_list:\n",
    "        account_node_mapping[account] = node_idx\n",
    "        node_types.append(0)\n",
    "        features = account_features[account]\n",
    "        padded_features = features + [0.0] * (20 - len(features))\n",
    "        node_features_list.append(padded_features[:20])\n",
    "        node_idx += 1\n",
    "    \n",
    "    bank_stats = transactions_df.groupby('from_bank').agg({\n",
    "        'amount_paid': ['count', 'sum', 'mean'],\n",
    "        'is_laundering': 'sum',\n",
    "        'payment_currency': 'nunique',\n",
    "        'payment_format': 'nunique'\n",
    "    }).fillna(0)\n",
    "    \n",
    "    bank_stats.columns = ['_'.join(col) for col in bank_stats.columns]\n",
    "    \n",
    "    for bank in bank_list:\n",
    "        bank_node_mapping[bank] = node_idx\n",
    "        node_types.append(1)\n",
    "        \n",
    "        if bank in bank_stats.index:\n",
    "            stats = bank_stats.loc[bank]\n",
    "            bank_features = [\n",
    "                float(stats.get('amount_paid_count', 0)),\n",
    "                float(stats.get('amount_paid_sum', 0)),\n",
    "                float(stats.get('amount_paid_mean', 0)),\n",
    "                float(stats.get('is_laundering_sum', 0)),\n",
    "                float(stats.get('payment_currency_nunique', 0)),\n",
    "                float(stats.get('payment_format_nunique', 0))\n",
    "            ]\n",
    "        else:\n",
    "            bank_features = [0.0] * 6\n",
    "        \n",
    "        padded_features = bank_features + [0.0] * (20 - len(bank_features))\n",
    "        node_features_list.append(padded_features[:20])\n",
    "        node_idx += 1\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    payment_encoder = constructor.preprocessors['payment_encoder']\n",
    "    currency_encoder = constructor.preprocessors['currency_encoder']\n",
    "    \n",
    "    sample_size = min(1000000, len(transactions_df))\n",
    "    sampled_indices = np.random.choice(len(transactions_df), sample_size, replace=False)\n",
    "    sampled_transactions = transactions_df.iloc[sampled_indices]\n",
    "    \n",
    "    for _, txn in tqdm(sampled_transactions.iterrows(), total=len(sampled_transactions), desc=\"Building multimodal edges\"):\n",
    "        if txn['account_origin'] in account_node_mapping and txn['account_destination'] in account_node_mapping:\n",
    "            source_account_idx = account_node_mapping[txn['account_origin']]\n",
    "            target_account_idx = account_node_mapping[txn['account_destination']]\n",
    "            \n",
    "            edge_list.append([source_account_idx, target_account_idx])\n",
    "            edge_attributes.append([\n",
    "                float(txn['amount_paid']),\n",
    "                float(payment_encoder.transform([txn['payment_format']])[0]),\n",
    "                float(currency_encoder.transform([txn['payment_currency']])[0]),\n",
    "                0.0\n",
    "            ])\n",
    "            \n",
    "            if txn['from_bank'] in bank_node_mapping:\n",
    "                from_bank_idx = bank_node_mapping[txn['from_bank']]\n",
    "                edge_list.append([source_account_idx, from_bank_idx])\n",
    "                edge_attributes.append([0.0, 0.0, 0.0, 1.0])\n",
    "            \n",
    "            if txn['to_bank'] in bank_node_mapping:\n",
    "                to_bank_idx = bank_node_mapping[txn['to_bank']]\n",
    "                edge_list.append([target_account_idx, to_bank_idx])\n",
    "                edge_attributes.append([0.0, 0.0, 0.0, 1.0])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float32)\n",
    "    \n",
    "    scaler_multimodal = StandardScaler()\n",
    "    node_features_scaled = scaler_multimodal.fit_transform(node_features_list)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float32)\n",
    "    node_type = torch.tensor(node_types, dtype=torch.long)\n",
    "    \n",
    "    ml_accounts = set(transactions_df[transactions_df['is_laundering'] == 1]['account_origin']) | set(transactions_df[transactions_df['is_laundering'] == 1]['account_destination'])\n",
    "    ml_banks = set(transactions_df[transactions_df['is_laundering'] == 1]['from_bank']) | set(transactions_df[transactions_df['is_laundering'] == 1]['to_bank'])\n",
    "    \n",
    "    node_labels = []\n",
    "    for account in account_list:\n",
    "        node_labels.append(1 if account in ml_accounts else 0)\n",
    "    for bank in bank_list:\n",
    "        node_labels.append(1 if bank in ml_banks else 0)\n",
    "    \n",
    "    y = torch.tensor(node_labels, dtype=torch.long)\n",
    "    \n",
    "    multimodal_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type,\n",
    "        num_nodes=len(node_features_list)\n",
    "    )\n",
    "    \n",
    "    graph_data = {\n",
    "        'graph': multimodal_graph,\n",
    "        'preprocessors': {\n",
    "            'multimodal_scaler': scaler_multimodal\n",
    "        },\n",
    "        'mappings': {\n",
    "            'account_node_mapping': account_node_mapping,\n",
    "            'bank_node_mapping': bank_node_mapping\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['multimodal_integration'] = multimodal_graph\n",
    "    constructor.preprocessors.update(graph_data['preprocessors'])\n",
    "    constructor.save_checkpoint('multimodal_integration_graph', graph_data)\n",
    "    constructor.monitor_memory('multimodal_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17b56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ground_truth_pattern_graph(constructor, data, account_features_data):\n",
    "    graph_data = constructor.load_checkpoint('ground_truth_pattern_graph')\n",
    "    if graph_data is not None:\n",
    "        if graph_data['graph'] is not None:\n",
    "            constructor.graphs['ground_truth_patterns'] = graph_data['graph']\n",
    "        return\n",
    "    \n",
    "    print(\"Building ground truth pattern graph...\")\n",
    "    pattern_transactions_df = data['pattern_transactions_df']\n",
    "    node_mappings = constructor.node_mappings\n",
    "    account_features = account_features_data['account_features']\n",
    "    \n",
    "    if len(pattern_transactions_df) == 0:\n",
    "        print(\"No pattern transactions available\")\n",
    "        constructor.save_checkpoint('ground_truth_pattern_graph', {'graph': None})\n",
    "        return\n",
    "    \n",
    "    pattern_accounts = set(pattern_transactions_df['account_origin'].unique()) | set(pattern_transactions_df['account_destination'].unique())\n",
    "    pattern_accounts = [acc for acc in pattern_accounts if acc in node_mappings['accounts']]\n",
    "    \n",
    "    if len(pattern_accounts) == 0:\n",
    "        print(\"No pattern accounts found in node mappings\")\n",
    "        constructor.save_checkpoint('ground_truth_pattern_graph', {'graph': None})\n",
    "        return\n",
    "    \n",
    "    pattern_account_mapping = {acc: idx for idx, acc in enumerate(pattern_accounts)}\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    pattern_types = pattern_transactions_df['pattern_type'].unique()\n",
    "    pattern_encoder = LabelEncoder()\n",
    "    pattern_encoder.fit(pattern_types)\n",
    "    \n",
    "    for _, txn in tqdm(pattern_transactions_df.iterrows(), total=len(pattern_transactions_df), desc=\"Processing pattern transactions\"):\n",
    "        if (txn['account_origin'] in pattern_account_mapping and \n",
    "            txn['account_destination'] in pattern_account_mapping):\n",
    "            \n",
    "            source_idx = pattern_account_mapping[txn['account_origin']]\n",
    "            target_idx = pattern_account_mapping[txn['account_destination']]\n",
    "            \n",
    "            edge_list.append([source_idx, target_idx])\n",
    "            edge_attributes.append([\n",
    "                float(pattern_encoder.transform([txn['pattern_type']])[0]),\n",
    "                float(txn.get('txn_sequence', 1)),\n",
    "                float(txn.get('total_txns_in_pattern', 1)),\n",
    "                float(txn['amount_paid'])\n",
    "            ])\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        print(\"No edges found in pattern graph\")\n",
    "        constructor.save_checkpoint('ground_truth_pattern_graph', {'graph': None})\n",
    "        return\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float32)\n",
    "    \n",
    "    node_features = []\n",
    "    for account in pattern_accounts:\n",
    "        features = account_features[account]\n",
    "        node_features.append(features)\n",
    "    \n",
    "    scaler_pattern = StandardScaler()\n",
    "    node_features_scaled = scaler_pattern.fit_transform(node_features)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float32)\n",
    "    y = torch.ones(len(pattern_accounts), dtype=torch.long)\n",
    "    \n",
    "    pattern_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(pattern_accounts)\n",
    "    )\n",
    "    \n",
    "    graph_data = {\n",
    "        'graph': pattern_graph,\n",
    "        'pattern_account_mapping': pattern_account_mapping\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['ground_truth_patterns'] = pattern_graph\n",
    "    constructor.save_checkpoint('ground_truth_pattern_graph', graph_data)\n",
    "    constructor.monitor_memory('pattern_graph_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb090af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ego_network_framework(constructor):\n",
    "    ego_data = constructor.load_checkpoint('ego_networks')\n",
    "    if ego_data is not None:\n",
    "        constructor.graphs['ego_networks'] = ego_data\n",
    "        return\n",
    "    \n",
    "    print(\"Creating ego network extraction framework...\")\n",
    "    \n",
    "    if 'transaction_flow' not in constructor.graphs:\n",
    "        print(\"Transaction flow graph not available for ego networks\")\n",
    "        constructor.save_checkpoint('ego_networks', {})\n",
    "        return\n",
    "    \n",
    "    transaction_flow_graph = constructor.graphs['transaction_flow']\n",
    "    node_mappings = constructor.node_mappings\n",
    "    \n",
    "    def extract_ego_network(target_account, k_hops=2):\n",
    "        if target_account not in node_mappings['accounts']:\n",
    "            return None\n",
    "        \n",
    "        target_idx = node_mappings['accounts'][target_account]\n",
    "        edge_index = transaction_flow_graph.edge_index\n",
    "        \n",
    "        current_nodes = {target_idx}\n",
    "        \n",
    "        for hop in range(k_hops):\n",
    "            next_nodes = set()\n",
    "            for node in current_nodes:\n",
    "                neighbors_out = edge_index[1][edge_index[0] == node].tolist()\n",
    "                neighbors_in = edge_index[0][edge_index[1] == node].tolist()\n",
    "                next_nodes.update(neighbors_out + neighbors_in)\n",
    "            current_nodes.update(next_nodes)\n",
    "        \n",
    "        ego_nodes = sorted(list(current_nodes))\n",
    "        \n",
    "        if len(ego_nodes) <= 1:\n",
    "            return None\n",
    "        \n",
    "        node_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(ego_nodes)}\n",
    "        \n",
    "        ego_edge_list = []\n",
    "        ego_edge_attrs = []\n",
    "        \n",
    "        for i, (src, dst) in enumerate(edge_index.t().tolist()):\n",
    "            if src in node_mapping and dst in node_mapping:\n",
    "                ego_edge_list.append([node_mapping[src], node_mapping[dst]])\n",
    "                ego_edge_attrs.append(transaction_flow_graph.edge_attr[i].tolist())\n",
    "        \n",
    "        if len(ego_edge_list) == 0:\n",
    "            return None\n",
    "        \n",
    "        ego_edge_index = torch.tensor(ego_edge_list, dtype=torch.long).t().contiguous()\n",
    "        ego_edge_attr = torch.tensor(ego_edge_attrs, dtype=torch.float32)\n",
    "        \n",
    "        ego_x = transaction_flow_graph.x[ego_nodes]\n",
    "        ego_y = transaction_flow_graph.y[ego_nodes]\n",
    "        \n",
    "        ego_graph = Data(\n",
    "            x=ego_x,\n",
    "            edge_index=ego_edge_index,\n",
    "            edge_attr=ego_edge_attr,\n",
    "            y=ego_y,\n",
    "            num_nodes=len(ego_nodes)\n",
    "        )\n",
    "        \n",
    "        return ego_graph\n",
    "    \n",
    "    sample_ego_networks = {}\n",
    "    ml_accounts = []\n",
    "    \n",
    "    if 'transaction_flow' in constructor.graphs:\n",
    "        account_list = sorted(node_mappings['accounts'].keys())\n",
    "        for i, label in enumerate(constructor.graphs['transaction_flow'].y):\n",
    "            if label == 1:\n",
    "                ml_accounts.append(account_list[i])\n",
    "    \n",
    "    for account in ml_accounts[:5]:\n",
    "        ego_result = extract_ego_network(account)\n",
    "        if ego_result is not None:\n",
    "            sample_ego_networks[account] = ego_result\n",
    "            print(f\"Created ego network for {account}: {ego_result.num_nodes} nodes\")\n",
    "    \n",
    "    ego_data = {\n",
    "        'sample_networks': sample_ego_networks,\n",
    "        'extractor_function': extract_ego_network\n",
    "    }\n",
    "    \n",
    "    constructor.graphs['ego_networks'] = ego_data\n",
    "    constructor.save_checkpoint('ego_networks', ego_data)\n",
    "    constructor.monitor_memory('ego_networks_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a134f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_graphs_and_metadata(constructor):\n",
    "   print(\"Saving all graphs and comprehensive metadata...\")\n",
    "   \n",
    "   graph_save_data = {}\n",
    "   for graph_name, graph_obj in constructor.graphs.items():\n",
    "       if graph_obj is not None:\n",
    "           if graph_name == 'ego_networks':\n",
    "               graph_save_data['sample_ego_networks'] = graph_obj.get('sample_networks', {})\n",
    "           else:\n",
    "               graph_save_data[f'{graph_name}_graph'] = graph_obj\n",
    "   \n",
    "   torch.save(graph_save_data, 'data/optimized_graphs.pt')\n",
    "   \n",
    "   preprocessor_data = {\n",
    "       'preprocessors': constructor.preprocessors,\n",
    "       'metadata': {\n",
    "           'creation_timestamp': constructor.creation_timestamp,\n",
    "           'version': constructor.version,\n",
    "           'optimization_applied': True,\n",
    "           'memory_efficient': True\n",
    "       }\n",
    "   }\n",
    "   \n",
    "   torch.save(preprocessor_data, 'data/optimized_preprocessors.pt')\n",
    "   \n",
    "   comprehensive_metadata = {\n",
    "       'node_mappings': constructor.node_mappings,\n",
    "       'creation_info': {\n",
    "           'timestamp': constructor.creation_timestamp,\n",
    "           'version': constructor.version,\n",
    "           'graphs_created': len([g for g in constructor.graphs.values() if g is not None]),\n",
    "           'optimization_level': 'high_performance'\n",
    "       }\n",
    "   }\n",
    "   \n",
    "   with open('data/optimized_metadata.pkl', 'wb') as f:\n",
    "       pickle.dump(comprehensive_metadata, f)\n",
    "   \n",
    "   summary = {\n",
    "       'total_graphs': len([g for g in constructor.graphs.values() if g is not None]),\n",
    "       'optimization_applied': True,\n",
    "       'memory_usage_reduced': True,\n",
    "       'checkpointing_enabled': True,\n",
    "       'files_created': [\n",
    "           'data/optimized_graphs.pt',\n",
    "           'data/optimized_preprocessors.pt', \n",
    "           'data/optimized_metadata.pkl'\n",
    "       ]\n",
    "   }\n",
    "   \n",
    "   print(\"All graphs saved successfully!\")\n",
    "   print(f\"Created {summary['total_graphs']} optimized graphs\")\n",
    "   \n",
    "   return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "252505b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optimized Stage 3: Graph Construction with Checkpointing\n",
      "================================================================================\n",
      "Resuming from checkpoint: temporal_proximity_graph\n",
      "Loaded checkpoint: account_features_batched\n",
      "Building efficient account behavior graph...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accounts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m constructor, summary\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     constructor, summary \u001b[38;5;241m=\u001b[39m \u001b[43mrun_optimized_stage3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mrun_optimized_stage3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     build_sampled_temporal_proximity_graph(constructor, data)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_point \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m resume_point \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_loaded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_mappings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_features_batched\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_flow_graph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporal_proximity_graph\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mbuild_efficient_account_behavior_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_features_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_point \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m resume_point \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_loaded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_mappings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_features_batched\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_flow_graph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporal_proximity_graph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_behavior_graph\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     29\u001b[0m     data \u001b[38;5;241m=\u001b[39m constructor\u001b[38;5;241m.\u001b[39mload_checkpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_loaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mbuild_efficient_account_behavior_graph\u001b[0;34m(constructor, account_features_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m account_features \u001b[38;5;241m=\u001b[39m account_features_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m node_mappings \u001b[38;5;241m=\u001b[39m constructor\u001b[38;5;241m.\u001b[39mnode_mappings\n\u001b[0;32m---> 12\u001b[0m account_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mnode_mappings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccounts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     14\u001b[0m feature_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([account_features[acc] \u001b[38;5;28;01mfor\u001b[39;00m acc \u001b[38;5;129;01min\u001b[39;00m account_list], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     16\u001b[0m scaler_behavior \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accounts'"
     ]
    }
   ],
   "source": [
    "def run_optimized_stage3():\n",
    "    print(\"Starting Optimized Stage 3: Graph Construction with Checkpointing\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    constructor, resume_point = load_and_prepare_data()\n",
    "    \n",
    "    if resume_point is None or resume_point == 'data_loaded':\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        create_optimized_node_mappings(constructor, data)\n",
    "        \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings']:\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        account_features_data = engineer_account_features_batched(constructor, data)\n",
    "    else:\n",
    "        account_features_data = constructor.load_checkpoint('account_features_batched')\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched']:\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        build_optimized_transaction_flow_graph(constructor, data, account_features_data)\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched', 'transaction_flow_graph']:\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        build_sampled_temporal_proximity_graph(constructor, data)\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched', 'transaction_flow_graph', 'temporal_proximity_graph']:\n",
    "        build_efficient_account_behavior_graph(constructor, account_features_data)\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched', 'transaction_flow_graph', 'temporal_proximity_graph', 'account_behavior_graph']:\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        build_optimized_multimodal_graph(constructor, data, account_features_data)\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched', 'transaction_flow_graph', 'temporal_proximity_graph', 'account_behavior_graph', 'multimodal_integration_graph']:\n",
    "        data = constructor.load_checkpoint('data_loaded')\n",
    "        build_ground_truth_pattern_graph(constructor, data, account_features_data)\n",
    "    \n",
    "    if resume_point is None or resume_point in ['data_loaded', 'node_mappings', 'account_features_batched', 'transaction_flow_graph', 'temporal_proximity_graph', 'account_behavior_graph', 'multimodal_integration_graph', 'ground_truth_pattern_graph']:\n",
    "        create_ego_network_framework(constructor)\n",
    "    \n",
    "    summary = save_all_graphs_and_metadata(constructor)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZED STAGE 3 COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Created {summary['total_graphs']} optimized graphs\")\n",
    "    print(\"Memory usage optimized with batching and sampling\")\n",
    "    print(\"Checkpointing enabled for resumable execution\") \n",
    "    print(\"All graphs saved to optimized format\")\n",
    "    print(\"Ready for Stage 4: Explainable GAT Architecture\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return constructor, summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    constructor, summary = run_optimized_stage3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
