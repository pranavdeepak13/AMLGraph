{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5,078,345 transactions\n",
      "Loaded 518,581 accounts\n",
      "Loaded 370 patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.utils import to_undirected\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "transactions_df = pd.read_csv('../data/processed_transactions.csv')\n",
    "accounts_df = pd.read_csv('../data/processed_accounts.csv')\n",
    "patterns_df = pd.read_csv('../data/processed_patterns.csv')\n",
    "pattern_transactions_df = pd.read_csv('../data/processed_pattern_transactions.csv')\n",
    "\n",
    "transactions_df['timestamp'] = pd.to_datetime(transactions_df['timestamp'])\n",
    "pattern_transactions_df['timestamp'] = pd.to_datetime(pattern_transactions_df['timestamp'])\n",
    "\n",
    "with open('../data/insights_summary.pkl', 'rb') as f:\n",
    "    insights_summary = pickle.load(f)\n",
    "\n",
    "with open('../data/network_metrics.pkl', 'rb') as f:\n",
    "    network_metrics = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(transactions_df):,} transactions\")\n",
    "print(f\"Loaded {len(accounts_df):,} accounts\") \n",
    "print(f\"Loaded {len(patterns_df)} patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47ab55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive graph annotation system...\n",
      "Creating annotated node mappings...\n",
      "Mapped 515,080 accounts\n",
      "Mapped 30,470 banks\n",
      "Mapped 5,078,345 transactions\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating comprehensive graph annotation system...\")\n",
    "\n",
    "class AnnotatedGraphConstructor:\n",
    "    def __init__(self, transactions_df, accounts_df, pattern_transactions_df, insights_summary):\n",
    "        self.transactions_df = transactions_df\n",
    "        self.accounts_df = accounts_df\n",
    "        self.pattern_transactions_df = pattern_transactions_df\n",
    "        self.insights = insights_summary\n",
    "        \n",
    "        self.node_mappings = {}\n",
    "        self.account_features = {}\n",
    "        self.graphs = {}\n",
    "        self.annotations = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "        self.creation_timestamp = datetime.now().isoformat()\n",
    "        self.version = \"1.0\"\n",
    "        \n",
    "    def create_comprehensive_annotations(self):\n",
    "        \"\"\"Create detailed annotations for all graph components\"\"\"\n",
    "        self.annotations = {\n",
    "            'project_info': {\n",
    "                'name': 'Explainable AML Detection with Graph Neural Networks',\n",
    "                'version': self.version,\n",
    "                'creation_date': self.creation_timestamp,\n",
    "                'description': 'Multi-modal graph construction for explainable anti-money laundering detection',\n",
    "                'dataset': 'IBM Synthetic AML Dataset (HI-Small)',\n",
    "                'purpose': 'Regulatory-compliant explainable AI for financial crime detection'\n",
    "            },\n",
    "            'data_sources': {\n",
    "                'transactions': {\n",
    "                    'file': 'processed_transactions.csv',\n",
    "                    'rows': len(self.transactions_df),\n",
    "                    'columns': list(self.transactions_df.columns),\n",
    "                    'date_range': f\"{self.transactions_df['timestamp'].min()} to {self.transactions_df['timestamp'].max()}\",\n",
    "                    'ml_rate': self.transactions_df['is_laundering'].mean(),\n",
    "                    'description': 'Standardized transaction data with temporal and amount features'\n",
    "                },\n",
    "                'accounts': {\n",
    "                    'file': 'processed_accounts.csv',\n",
    "                    'rows': len(self.accounts_df),\n",
    "                    'columns': list(self.accounts_df.columns),\n",
    "                    'unique_banks': self.accounts_df['bank_id'].nunique(),\n",
    "                    'unique_entities': self.accounts_df['entity_id'].nunique(),\n",
    "                    'description': 'Account information with entity types and banking relationships'\n",
    "                },\n",
    "                'patterns': {\n",
    "                    'file': 'processed_pattern_transactions.csv',\n",
    "                    'rows': len(self.pattern_transactions_df),\n",
    "                    'pattern_types': list(self.pattern_transactions_df['pattern_type'].unique()) if len(self.pattern_transactions_df) > 0 else [],\n",
    "                    'description': 'Ground truth money laundering patterns for validation'\n",
    "                }\n",
    "            },\n",
    "            'methodology': {\n",
    "                'graph_construction_approach': 'Multi-modal heterogeneous graph construction',\n",
    "                'feature_engineering': 'Behavioral pattern analysis with temporal and structural features',\n",
    "                'explainability_focus': 'Graph Attention Network compatible structure for regulatory explanations',\n",
    "                'validation_strategy': 'Ground truth pattern validation with ego-network analysis'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return self.annotations\n",
    "    \n",
    "    def create_node_mappings_with_annotations(self):\n",
    "        \"\"\"Create node mappings with detailed annotations\"\"\"\n",
    "        print(\"Creating annotated node mappings...\")\n",
    "        \n",
    "        unique_accounts = set()\n",
    "        unique_accounts.update(self.transactions_df['account_origin'].unique())\n",
    "        unique_accounts.update(self.transactions_df['account_destination'].unique())\n",
    "        \n",
    "        self.node_mappings['accounts'] = {acc: idx for idx, acc in enumerate(sorted(unique_accounts))}\n",
    "        \n",
    "        unique_banks = set()\n",
    "        unique_banks.update(self.transactions_df['from_bank'].unique())\n",
    "        unique_banks.update(self.transactions_df['to_bank'].unique())\n",
    "        \n",
    "        self.node_mappings['banks'] = {bank: idx for idx, bank in enumerate(sorted(unique_banks))}\n",
    "        \n",
    "        self.node_mappings['transactions'] = {idx: idx for idx in self.transactions_df.index}\n",
    "        \n",
    "        self.annotations['node_mappings'] = {\n",
    "            'accounts': {\n",
    "                'count': len(self.node_mappings['accounts']),\n",
    "                'description': 'Bank account identifiers mapped to sequential integers',\n",
    "                'mapping_strategy': 'Sorted alphabetical order for consistency',\n",
    "                'usage': 'Primary entities in transaction flow and behavior graphs'\n",
    "            },\n",
    "            'banks': {\n",
    "                'count': len(self.node_mappings['banks']),\n",
    "                'description': 'Bank identifiers mapped to sequential integers',\n",
    "                'mapping_strategy': 'Sorted alphabetical order for consistency',\n",
    "                'usage': 'Institutional nodes in multi-modal integration graph'\n",
    "            },\n",
    "            'transactions': {\n",
    "                'count': len(self.node_mappings['transactions']),\n",
    "                'description': 'Transaction indices for temporal proximity graph',\n",
    "                'mapping_strategy': 'Original dataframe indices preserved',\n",
    "                'usage': 'Transaction-level nodes for sequence analysis'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Mapped {len(self.node_mappings['accounts']):,} accounts\")\n",
    "        print(f\"Mapped {len(self.node_mappings['banks']):,} banks\")\n",
    "        print(f\"Mapped {len(self.node_mappings['transactions']):,} transactions\")\n",
    "        \n",
    "        return self.node_mappings\n",
    "\n",
    "annotated_constructor = AnnotatedGraphConstructor(transactions_df, accounts_df, pattern_transactions_df, insights_summary)\n",
    "annotations = annotated_constructor.create_comprehensive_annotations()\n",
    "node_mappings = annotated_constructor.create_node_mappings_with_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfdb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering annotated account-level behavioral features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering annotated account-level behavioral features...\")\n",
    "\n",
    "def engineer_annotated_account_features(transactions_df, accounts_df, node_mappings, annotated_constructor):\n",
    "    \"\"\"Engineer account features with comprehensive annotations\"\"\"\n",
    "    account_features = {}\n",
    "    \n",
    "    feature_definitions = {\n",
    "        'total_outgoing': 'Number of outgoing transactions from account',\n",
    "        'total_incoming': 'Number of incoming transactions to account', \n",
    "        'total_transactions': 'Total transaction count (outgoing + incoming)',\n",
    "        'avg_outgoing_amount': 'Average amount of outgoing transactions (USD)',\n",
    "        'avg_incoming_amount': 'Average amount of incoming transactions (USD)',\n",
    "        'total_outgoing_volume': 'Total volume of outgoing transactions (USD)',\n",
    "        'total_incoming_volume': 'Total volume of incoming transactions (USD)',\n",
    "        'unique_recipients': 'Number of unique recipient accounts (fan-out indicator)',\n",
    "        'unique_senders': 'Number of unique sender accounts (fan-in indicator)',\n",
    "        'currency_diversity': 'Number of different currencies used',\n",
    "        'payment_format_diversity': 'Number of different payment formats used',\n",
    "        'transaction_velocity': 'Transactions per hour (velocity indicator)',\n",
    "        'fan_out_degree': 'Outgoing connection count (laundering pattern indicator)',\n",
    "        'fan_in_degree': 'Incoming connection count (collection pattern indicator)',\n",
    "        'ml_rate': 'Money laundering involvement rate (0-1)',\n",
    "        'round_amount_ratio': 'Proportion of round-number transactions (structuring indicator)',\n",
    "        'outgoing_time_span': 'Time span of outgoing transactions (hours)'\n",
    "    }\n",
    "    \n",
    "    feature_engineering_stats = {\n",
    "        'total_features': len(feature_definitions),\n",
    "        'categories': {\n",
    "            'transaction_counts': ['total_outgoing', 'total_incoming', 'total_transactions'],\n",
    "            'amount_statistics': ['avg_outgoing_amount', 'avg_incoming_amount', 'total_outgoing_volume', 'total_incoming_volume'],\n",
    "            'network_properties': ['unique_recipients', 'unique_senders', 'fan_out_degree', 'fan_in_degree'],\n",
    "            'behavioral_indicators': ['currency_diversity', 'payment_format_diversity', 'transaction_velocity'],\n",
    "            'risk_indicators': ['ml_rate', 'round_amount_ratio'],\n",
    "            'temporal_features': ['outgoing_time_span']\n",
    "        },\n",
    "        'interpretability_focus': 'All features designed for regulatory explanation and compliance auditing',\n",
    "        'missing_value_handling': 'Zero-filled for accounts with no transactions in specific directions'\n",
    "    }\n",
    "    \n",
    "    for account in node_mappings['accounts'].keys():\n",
    "        outgoing_txns = transactions_df[transactions_df['account_origin'] == account]\n",
    "        incoming_txns = transactions_df[transactions_df['account_destination'] == account]\n",
    "        \n",
    "        total_outgoing = len(outgoing_txns)\n",
    "        total_incoming = len(incoming_txns)\n",
    "        \n",
    "        if total_outgoing > 0:\n",
    "            avg_outgoing_amount = outgoing_txns['amount_paid'].mean()\n",
    "            total_outgoing_volume = outgoing_txns['amount_paid'].sum()\n",
    "            unique_recipients = outgoing_txns['account_destination'].nunique()\n",
    "            currency_diversity = outgoing_txns['payment_currency'].nunique()\n",
    "            payment_format_diversity = outgoing_txns['payment_format'].nunique()\n",
    "            ml_outgoing_count = outgoing_txns['is_laundering'].sum()\n",
    "            \n",
    "            outgoing_time_span = 0\n",
    "            if len(outgoing_txns) > 1:\n",
    "                outgoing_time_span = (outgoing_txns['timestamp'].max() - \n",
    "                                    outgoing_txns['timestamp'].min()).total_seconds() / 3600\n",
    "            \n",
    "            round_amounts = outgoing_txns['amount_paid'].apply(lambda x: x == round(x, -2)).sum()\n",
    "            round_amount_ratio = round_amounts / total_outgoing\n",
    "        else:\n",
    "            avg_outgoing_amount = 0\n",
    "            total_outgoing_volume = 0\n",
    "            unique_recipients = 0\n",
    "            currency_diversity = 0\n",
    "            payment_format_diversity = 0\n",
    "            ml_outgoing_count = 0\n",
    "            outgoing_time_span = 0\n",
    "            round_amount_ratio = 0\n",
    "        \n",
    "        if total_incoming > 0:\n",
    "            avg_incoming_amount = incoming_txns['amount_paid'].mean()\n",
    "            total_incoming_volume = incoming_txns['amount_paid'].sum()\n",
    "            unique_senders = incoming_txns['account_origin'].nunique()\n",
    "            ml_incoming_count = incoming_txns['is_laundering'].sum()\n",
    "        else:\n",
    "            avg_incoming_amount = 0\n",
    "            total_incoming_volume = 0\n",
    "            unique_senders = 0\n",
    "            ml_incoming_count = 0\n",
    "        \n",
    "        transaction_velocity = total_outgoing / max(outgoing_time_span, 1)\n",
    "        fan_out_degree = unique_recipients\n",
    "        fan_in_degree = unique_senders\n",
    "        ml_rate = (ml_outgoing_count + ml_incoming_count) / max(total_outgoing + total_incoming, 1)\n",
    "        \n",
    "        features = [\n",
    "            total_outgoing, total_incoming, total_outgoing + total_incoming,\n",
    "            avg_outgoing_amount, avg_incoming_amount,\n",
    "            total_outgoing_volume, total_incoming_volume,\n",
    "            unique_recipients, unique_senders,\n",
    "            currency_diversity, payment_format_diversity,\n",
    "            transaction_velocity, fan_out_degree, fan_in_degree,\n",
    "            ml_rate, round_amount_ratio, outgoing_time_span\n",
    "        ]\n",
    "        \n",
    "        account_features[account] = features\n",
    "    \n",
    "    feature_names = list(feature_definitions.keys())\n",
    "    \n",
    "    annotated_constructor.annotations['feature_engineering'] = {\n",
    "        'feature_definitions': feature_definitions,\n",
    "        'feature_names': feature_names,\n",
    "        'engineering_stats': feature_engineering_stats,\n",
    "        'total_accounts_processed': len(account_features),\n",
    "        'feature_vector_length': len(feature_names),\n",
    "        'normalization_note': 'Features require StandardScaler normalization before model training'\n",
    "    }\n",
    "    \n",
    "    return account_features, feature_names, feature_definitions\n",
    "\n",
    "account_features, feature_names, feature_definitions = engineer_annotated_account_features(\n",
    "    transactions_df, accounts_df, node_mappings, annotated_constructor\n",
    ")\n",
    "\n",
    "print(f\"Engineered {len(feature_names)} annotated behavioral features per account\")\n",
    "print(\"Feature categories created with full documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d74714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Transaction Flow Graph...\")\n",
    "\n",
    "def build_annotated_transaction_flow_graph(transactions_df, node_mappings, account_features, annotated_constructor):\n",
    "    \"\"\"Build transaction flow graph with comprehensive annotations\"\"\"\n",
    "    \n",
    "    graph_annotation = {\n",
    "        'graph_name': 'Transaction Flow Graph',\n",
    "        'graph_type': 'Homogeneous Directed Graph',\n",
    "        'purpose': 'Direct money transfer pattern detection for fan-out, fan-in, and layering schemes',\n",
    "        'node_type': 'Bank Accounts',\n",
    "        'edge_type': 'Direct Money Transfers',\n",
    "        'construction_method': 'One edge per transaction between accounts',\n",
    "        'key_applications': [\n",
    "            'Fan-out pattern detection (single source to multiple destinations)',\n",
    "            'Fan-in pattern detection (multiple sources to single destination)', \n",
    "            'Direct layering scheme identification',\n",
    "            'Account-level risk scoring',\n",
    "            'Transaction flow visualization for investigators'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Supports SAR (Suspicious Activity Report) generation with clear transaction chains'\n",
    "    }\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    payment_format_encoder = LabelEncoder()\n",
    "    currency_encoder = LabelEncoder()\n",
    "    \n",
    "    all_payment_formats = transactions_df['payment_format'].unique()\n",
    "    all_currencies = transactions_df['payment_currency'].unique()\n",
    "    \n",
    "    payment_format_encoder.fit(all_payment_formats)\n",
    "    currency_encoder.fit(all_currencies)\n",
    "    \n",
    "    edge_feature_definitions = {\n",
    "        'amount_paid': 'Transaction amount in original currency (log-normalized recommended)',\n",
    "        'amount_received': 'Transaction amount received (for currency conversion analysis)',\n",
    "        'payment_format_encoded': f'Payment method encoded (0-{len(all_payment_formats)-1}): {list(all_payment_formats)}',\n",
    "        'currency_encoded': f'Currency encoded (0-{len(all_currencies)-1}): {list(all_currencies)[:10]}...',\n",
    "        'hour_of_day': 'Transaction hour (0-23) for temporal pattern analysis',\n",
    "        'day_of_week': 'Day of week (0-6) for weekly pattern analysis',\n",
    "        'is_laundering': 'Ground truth money laundering label (0=normal, 1=ML)'\n",
    "    }\n",
    "    \n",
    "    for _, txn in transactions_df.iterrows():\n",
    "        if txn['account_origin'] in node_mappings['accounts'] and txn['account_destination'] in node_mappings['accounts']:\n",
    "            source_idx = node_mappings['accounts'][txn['account_origin']]\n",
    "            target_idx = node_mappings['accounts'][txn['account_destination']]\n",
    "            \n",
    "            edge_list.append([source_idx, target_idx])\n",
    "            \n",
    "            hour_of_day = txn['timestamp'].hour\n",
    "            day_of_week = txn['timestamp'].dayofweek\n",
    "            \n",
    "            edge_attr = [\n",
    "                txn['amount_paid'],\n",
    "                txn['amount_received'], \n",
    "                payment_format_encoder.transform([txn['payment_format']])[0],\n",
    "                currency_encoder.transform([txn['payment_currency']])[0],\n",
    "                hour_of_day,\n",
    "                day_of_week,\n",
    "                int(txn['is_laundering'])\n",
    "            ]\n",
    "            edge_attributes.append(edge_attr)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float)\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    node_features = []\n",
    "    node_labels = []\n",
    "    \n",
    "    for account in account_list:\n",
    "        features = account_features[account]\n",
    "        node_features.append(features)\n",
    "        \n",
    "        account_ml_involvement = transactions_df[\n",
    "            (transactions_df['account_origin'] == account) | \n",
    "            (transactions_df['account_destination'] == account)\n",
    "        ]['is_laundering'].sum()\n",
    "        \n",
    "        node_labels.append(1 if account_ml_involvement > 0 else 0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    node_features_scaled = scaler.fit_transform(node_features)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "    y = torch.tensor(node_labels, dtype=torch.long)\n",
    "    \n",
    "    transaction_flow_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(account_list)\n",
    "    )\n",
    "    \n",
    "    graph_stats = {\n",
    "        'nodes': transaction_flow_graph.num_nodes,\n",
    "        'edges': transaction_flow_graph.edge_index.size(1),\n",
    "        'node_features': transaction_flow_graph.x.size(1),\n",
    "        'edge_features': transaction_flow_graph.edge_attr.size(1),\n",
    "        'ml_accounts': y.sum().item(),\n",
    "        'ml_account_rate': (y.sum().item() / len(account_list)),\n",
    "        'avg_degree': (edge_index.size(1) / len(account_list)),\n",
    "        'self_loops': (edge_index[0] == edge_index[1]).sum().item(),\n",
    "        'density': edge_index.size(1) / (len(account_list) * (len(account_list) - 1))\n",
    "    }\n",
    "    \n",
    "    graph_annotation.update({\n",
    "        'node_features': feature_definitions,\n",
    "        'edge_features': edge_feature_definitions,\n",
    "        'statistics': graph_stats,\n",
    "        'preprocessing': {\n",
    "            'node_feature_scaling': 'StandardScaler applied',\n",
    "            'edge_feature_encoding': 'LabelEncoder for categorical variables',\n",
    "            'missing_value_handling': 'Zero-filled for inactive accounts'\n",
    "        },\n",
    "        'usage_recommendations': {\n",
    "            'training': 'Primary graph for basic GNN training and validation',\n",
    "            'explanation': 'Use for account-level risk explanations and direct transfer analysis',\n",
    "            'investigation': 'Extract ego-networks around suspicious accounts for detailed analysis',\n",
    "            'validation': 'Compare predictions with ground truth ML labels'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['graphs'] = annotated_constructor.annotations.get('graphs', {})\n",
    "    annotated_constructor.annotations['graphs']['transaction_flow'] = graph_annotation\n",
    "    \n",
    "    print(f\"Transaction Flow Graph created with annotations:\")\n",
    "    print(f\"  Nodes: {graph_stats['nodes']:,}\")\n",
    "    print(f\"  Edges: {graph_stats['edges']:,}\")\n",
    "    print(f\"  ML accounts: {graph_stats['ml_accounts']:,} ({graph_stats['ml_account_rate']:.4f})\")\n",
    "    print(f\"  Average degree: {graph_stats['avg_degree']:.2f}\")\n",
    "    \n",
    "    return transaction_flow_graph, scaler, payment_format_encoder, currency_encoder, graph_annotation\n",
    "\n",
    "transaction_flow_graph, feature_scaler, payment_encoder, currency_encoder, tf_annotation = build_annotated_transaction_flow_graph(\n",
    "    transactions_df, node_mappings, account_features, annotated_constructor\n",
    ")\n",
    "\n",
    "annotated_constructor.graphs['transaction_flow'] = transaction_flow_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5babbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Temporal Proximity Graph...\")\n",
    "\n",
    "def build_annotated_temporal_proximity_graph(transactions_df, payment_encoder, currency_encoder, annotated_constructor, time_window_hours=24):\n",
    "    \"\"\"Build temporal proximity graph with comprehensive annotations\"\"\"\n",
    "    \n",
    "    graph_annotation = {\n",
    "        'graph_name': 'Temporal Proximity Graph',\n",
    "        'graph_type': 'Homogeneous Undirected Graph',\n",
    "        'purpose': 'Complex multi-transaction money laundering scheme detection',\n",
    "        'node_type': 'Individual Transactions',\n",
    "        'edge_type': 'Temporal Relationships',\n",
    "        'construction_method': f'Connect transactions within {time_window_hours} hour windows',\n",
    "        'time_window_hours': time_window_hours,\n",
    "        'key_applications': [\n",
    "            'Scatter-gather pattern detection (rapid dispersion then collection)',\n",
    "            'Velocity pattern analysis (unusual transaction frequency)',\n",
    "            'Coordinated attack identification (simultaneous transactions)',\n",
    "            'Temporal clustering of suspicious activities',\n",
    "            'Transaction sequence analysis for complex schemes'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Enables detection of sophisticated multi-step laundering operations',\n",
    "        'computational_complexity': 'O(n*k) where n=transactions, k=average neighbors in time window'\n",
    "    }\n",
    "    \n",
    "    transactions_sorted = transactions_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    edge_feature_definitions = {\n",
    "        'time_gap_hours': f'Time difference between transactions (0-{time_window_hours} hours)',\n",
    "        'account_overlap': 'Binary indicator of shared accounts between transactions',\n",
    "        'amount_similarity': 'Normalized amount similarity (0-1, higher = more similar)',\n",
    "        'source_is_ml': 'Whether source transaction is money laundering (ground truth)',\n",
    "        'target_is_ml': 'Whether target transaction is money laundering (ground truth)'\n",
    "    }\n",
    "    \n",
    "    print(f\"Building temporal edges with {time_window_hours}h window...\")\n",
    "    \n",
    "    for i in range(len(transactions_sorted)):\n",
    "        current_txn = transactions_sorted.iloc[i]\n",
    "        current_time = current_txn['timestamp']\n",
    "        \n",
    "        window_start = i + 1\n",
    "        window_end = min(i + 1000, len(transactions_sorted))\n",
    "        \n",
    "        for j in range(window_start, window_end):\n",
    "            next_txn = transactions_sorted.iloc[j]\n",
    "            next_time = next_txn['timestamp']\n",
    "            \n",
    "            time_diff = (next_time - current_time).total_seconds() / 3600\n",
    "            \n",
    "            if time_diff > time_window_hours:\n",
    "                break\n",
    "            \n",
    "            account_overlap = 0\n",
    "            if (current_txn['account_origin'] == next_txn['account_origin'] or\n",
    "                current_txn['account_destination'] == next_txn['account_destination'] or\n",
    "                current_txn['account_origin'] == next_txn['account_destination'] or\n",
    "                current_txn['account_destination'] == next_txn['account_origin']):\n",
    "                account_overlap = 1\n",
    "            \n",
    "            amount_similarity = 1 - abs(current_txn['amount_paid'] - next_txn['amount_paid']) / max(\n",
    "                current_txn['amount_paid'] + next_txn['amount_paid'], 1)\n",
    "            \n",
    "            edge_list.append([i, j])\n",
    "            edge_attributes.append([\n",
    "                time_diff,\n",
    "                account_overlap,\n",
    "                amount_similarity,\n",
    "                int(current_txn['is_laundering']),\n",
    "                int(next_txn['is_laundering'])\n",
    "            ])\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Processed {i:,} transactions for temporal edges\")\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float)\n",
    "    \n",
    "    transaction_features = []\n",
    "    transaction_labels = []\n",
    "    \n",
    "    node_feature_definitions = {\n",
    "        'amount_paid': 'Transaction amount (requires log normalization)',\n",
    "        'hour_of_day': 'Transaction hour (0-23)',\n",
    "        'day_of_week': 'Day of week (0-6)',\n",
    "        'payment_format_encoded': 'Encoded payment method',\n",
    "        'currency_encoded': 'Encoded currency type'\n",
    "    }\n",
    "    \n",
    "    for _, txn in transactions_sorted.iterrows():\n",
    "        features = [\n",
    "            txn['amount_paid'],\n",
    "            txn['timestamp'].hour,\n",
    "            txn['timestamp'].dayofweek,\n",
    "            payment_encoder.transform([txn['payment_format']])[0],\n",
    "            currency_encoder.transform([txn['payment_currency']])[0]\n",
    "        ]\n",
    "        transaction_features.append(features)\n",
    "        transaction_labels.append(int(txn['is_laundering']))\n",
    "    \n",
    "    scaler_temporal = StandardScaler()\n",
    "    transaction_features_scaled = scaler_temporal.fit_transform(transaction_features)\n",
    "    \n",
    "    x = torch.tensor(transaction_features_scaled, dtype=torch.float)\n",
    "    y = torch.tensor(transaction_labels, dtype=torch.long)\n",
    "    \n",
    "    temporal_proximity_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(transactions_sorted)\n",
    "    )\n",
    "    \n",
    "    graph_stats = {\n",
    "        'nodes': temporal_proximity_graph.num_nodes,\n",
    "        'edges': temporal_proximity_graph.edge_index.size(1),\n",
    "        'node_features': temporal_proximity_graph.x.size(1),\n",
    "        'edge_features': temporal_proximity_graph.edge_attr.size(1),\n",
    "        'ml_transactions': y.sum().item(),\n",
    "        'ml_transaction_rate': (y.sum().item() / len(transactions_sorted)),\n",
    "        'avg_degree': (edge_index.size(1) / len(transactions_sorted)),\n",
    "        'avg_time_gap': torch.mean(edge_attr[:, 0]).item(),\n",
    "        'account_overlap_ratio': torch.mean(edge_attr[:, 1]).item()\n",
    "    }\n",
    "    \n",
    "    graph_annotation.update({\n",
    "        'node_features': node_feature_definitions,\n",
    "        'edge_features': edge_feature_definitions,\n",
    "        'statistics': graph_stats,\n",
    "        'preprocessing': {\n",
    "            'temporal_sorting': 'Transactions sorted chronologically for window construction',\n",
    "            'node_feature_scaling': 'StandardScaler applied to transaction features',\n",
    "            'edge_construction_optimization': 'Limited window search to prevent quadratic complexity'\n",
    "        },\n",
    "        'usage_recommendations': {\n",
    "            'training': 'Use for complex pattern detection requiring temporal sequence analysis',\n",
    "            'explanation': 'Identify rapid transaction sequences and coordinated timing patterns',\n",
    "            'investigation': 'Trace temporal relationships in suspected laundering schemes',\n",
    "            'validation': 'Verify model attention on temporally clustered ML transactions'\n",
    "        },\n",
    "        'memory_considerations': {\n",
    "            'large_graph_warning': 'High memory usage due to dense temporal connections',\n",
    "            'recommended_sampling': 'Use temporal subgraphs for training efficiency',\n",
    "            'batch_processing': 'Process in time-based chunks for large datasets'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['graphs']['temporal_proximity'] = graph_annotation\n",
    "    \n",
    "    print(f\"Temporal Proximity Graph created with annotations:\")\n",
    "    print(f\"  Nodes: {graph_stats['nodes']:,}\")\n",
    "    print(f\"  Edges: {graph_stats['edges']:,}\")\n",
    "    print(f\"  ML transactions: {graph_stats['ml_transactions']:,} ({graph_stats['ml_transaction_rate']:.4f})\")\n",
    "    print(f\"  Average time gap: {graph_stats['avg_time_gap']:.2f} hours\")\n",
    "    \n",
    "    return temporal_proximity_graph, scaler_temporal, graph_annotation\n",
    "\n",
    "temporal_proximity_graph, temporal_scaler, tp_annotation = build_annotated_temporal_proximity_graph(\n",
    "    transactions_df, payment_encoder, currency_encoder, annotated_constructor, time_window_hours=24\n",
    ")\n",
    "\n",
    "annotated_constructor.graphs['temporal_proximity'] = temporal_proximity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92675362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Account Behavior Graph...\")\n",
    "\n",
    "def build_annotated_account_behavior_graph(node_mappings, account_features, feature_definitions, annotated_constructor, similarity_threshold=0.7):\n",
    "    \"\"\"Build account behavior graph with comprehensive annotations\"\"\"\n",
    "    \n",
    "    graph_annotation = {\n",
    "        'graph_name': 'Account Behavior Graph',\n",
    "        'graph_type': 'Homogeneous Undirected Graph',\n",
    "        'purpose': 'Account risk profiling and criminal network identification',\n",
    "        'node_type': 'Bank Accounts with Behavioral Features',\n",
    "        'edge_type': 'Behavioral Similarity Connections',\n",
    "        'construction_method': f'Cosine similarity > {similarity_threshold} between behavioral feature vectors',\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "        'key_applications': [\n",
    "            'Money mule network identification (accounts with similar suspicious behavior)',\n",
    "            'Shell company network detection (coordinated fake entities)',\n",
    "            'Criminal organization structure mapping through behavioral patterns',\n",
    "            'Account-level risk scoring and clustering',\n",
    "            'Identification of coordinated account activities'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Supports network-based investigation and risk assessment for compliance teams'\n",
    "    }\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    \n",
    "    feature_matrix = []\n",
    "    for account in account_list:\n",
    "        feature_matrix.append(account_features[account])\n",
    "    \n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    \n",
    "    scaler_behavior = StandardScaler()\n",
    "    feature_matrix_scaled = scaler_behavior.fit_transform(feature_matrix)\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(feature_matrix_scaled)\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    print(f\"Computing behavioral similarities with threshold {similarity_threshold}...\")\n",
    "    \n",
    "    similarity_stats = {\n",
    "        'total_pairs_evaluated': len(account_list) * (len(account_list) - 1) // 2,\n",
    "        'edges_created': 0,\n",
    "        'avg_similarity': 0,\n",
    "        'max_similarity': 0\n",
    "    }\n",
    "    \n",
    "    all_similarities = []\n",
    "    \n",
    "    for i in range(len(account_list)):\n",
    "        for j in range(i + 1, len(account_list)):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            all_similarities.append(similarity)\n",
    "            \n",
    "            if similarity > similarity_threshold:\n",
    "                edge_list.extend([[i, j], [j, i]])\n",
    "                edge_attributes.extend([[similarity], [similarity]])\n",
    "                similarity_stats['edges_created'] += 2\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        print(f\"No edges found with threshold {similarity_threshold}, lowering to 0.5...\")\n",
    "        similarity_threshold = 0.5\n",
    "        \n",
    "        for i in range(len(account_list)):\n",
    "            for j in range(i + 1, len(account_list)):\n",
    "                similarity = similarity_matrix[i, j]\n",
    "                \n",
    "                if similarity > similarity_threshold:\n",
    "                    edge_list.extend([[i, j], [j, i]])\n",
    "                    edge_attributes.extend([[similarity], [similarity]])\n",
    "    \n",
    "    similarity_stats.update({\n",
    "        'avg_similarity': np.mean(all_similarities),\n",
    "        'max_similarity': np.max(all_similarities),\n",
    "        'similarity_threshold_used': similarity_threshold,\n",
    "        'connectivity_rate': len(edge_list) / (2 * similarity_stats['total_pairs_evaluated']) if similarity_stats['total_pairs_evaluated'] > 0 else 0\n",
    "    })\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float) if edge_attributes else torch.empty((0, 1), dtype=torch.float)\n",
    "    \n",
    "    x = torch.tensor(feature_matrix_scaled, dtype=torch.float)\n",
    "    \n",
    "    account_labels = []\n",
    "    for account in account_list:\n",
    "        account_ml_involvement = transactions_df[\n",
    "            (transactions_df['account_origin'] == account) | \n",
    "            (transactions_df['account_destination'] == account)\n",
    "        ]['is_laundering'].sum()\n",
    "        account_labels.append(1 if account_ml_involvement > 0 else 0)\n",
    "    \n",
    "    y = torch.tensor(account_labels, dtype=torch.long)\n",
    "    \n",
    "    account_behavior_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(account_list)\n",
    "    )\n",
    "    \n",
    "    edge_feature_definitions = {\n",
    "        'behavioral_similarity': f'Cosine similarity between behavioral feature vectors (range: {similarity_threshold}-1.0)'\n",
    "    }\n",
    "    \n",
    "    graph_stats = {\n",
    "        'nodes': account_behavior_graph.num_nodes,\n",
    "        'edges': account_behavior_graph.edge_index.size(1),\n",
    "        'node_features': account_behavior_graph.x.size(1),\n",
    "        'edge_features': account_behavior_graph.edge_attr.size(1),\n",
    "        'ml_accounts': y.sum().item(),\n",
    "        'ml_account_rate': (y.sum().item() / len(account_list)),\n",
    "        'avg_degree': (edge_index.size(1) / len(account_list)) if len(account_list) > 0 else 0,\n",
    "        'similarity_threshold_used': similarity_threshold,\n",
    "        'connectivity_metrics': similarity_stats\n",
    "    }\n",
    "    \n",
    "    graph_annotation.update({\n",
    "        'node_features': feature_definitions,\n",
    "        'edge_features': edge_feature_definitions,\n",
    "        'statistics': graph_stats,\n",
    "        'preprocessing': {\n",
    "            'feature_scaling': 'StandardScaler applied to behavioral features before similarity computation',\n",
    "            'similarity_metric': 'Cosine similarity for scale-invariant behavioral comparison',\n",
    "            'threshold_adaptation': 'Automatic threshold reduction if no edges found with initial threshold'\n",
    "        },\n",
    "        'usage_recommendations': {\n",
    "            'training': 'Use for network-based risk assessment and community detection',\n",
    "            'explanation': 'Identify similar behavioral patterns and account clustering',\n",
    "            'investigation': 'Map potential criminal networks through behavioral similarity',\n",
    "            'validation': 'Verify that ML accounts cluster together behaviorally'\n",
    "        },\n",
    "        'interpretation_guidelines': {\n",
    "            'high_similarity_edges': 'Indicate accounts with very similar transaction patterns',\n",
    "            'isolated_nodes': 'Accounts with unique behavioral signatures',\n",
    "            'dense_clusters': 'Potential coordinated account networks',\n",
    "            'ml_account_clustering': 'ML accounts should show higher interconnectivity'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['graphs']['account_behavior'] = graph_annotation\n",
    "    \n",
    "    print(f\"Account Behavior Graph created with annotations:\")\n",
    "    print(f\"  Nodes: {graph_stats['nodes']:,}\")\n",
    "    print(f\"  Edges: {graph_stats['edges']:,}\")\n",
    "    print(f\"  ML accounts: {graph_stats['ml_accounts']:,} ({graph_stats['ml_account_rate']:.4f})\")\n",
    "    print(f\"  Similarity threshold used: {similarity_threshold}\")\n",
    "    print(f\"  Average similarity: {similarity_stats['avg_similarity']:.4f}\")\n",
    "    \n",
    "    return account_behavior_graph, scaler_behavior, graph_annotation\n",
    "\n",
    "account_behavior_graph, behavior_scaler, ab_annotation = build_annotated_account_behavior_graph(\n",
    "    node_mappings, account_features, feature_definitions, annotated_constructor, similarity_threshold=0.7\n",
    "    )\n",
    "annotated_constructor.graphs['account_behavior'] = account_behavior_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Multi-Modal Integration Graph...\")\n",
    "\n",
    "def build_annotated_multimodal_integration_graph(transactions_df, node_mappings, account_features, feature_definitions, payment_encoder, currency_encoder, annotated_constructor):\n",
    "    \"\"\"Build multi-modal integration graph with comprehensive annotations\"\"\"\n",
    "    \n",
    "    graph_annotation = {\n",
    "        'graph_name': 'Multi-Modal Integration Graph',\n",
    "        'graph_type': 'Heterogeneous Directed Graph',\n",
    "        'purpose': 'Comprehensive AML detection with full institutional and behavioral context',\n",
    "        'node_types': ['Bank Accounts', 'Financial Institutions'],\n",
    "        'edge_types': ['Account-to-Account Transfers', 'Account-to-Bank Relationships'],\n",
    "        'construction_method': 'Heterogeneous graph combining multiple entity types and relationship types',\n",
    "        'key_applications': [\n",
    "            'Cross-institutional money laundering detection',\n",
    "            'Multi-bank coordination analysis',\n",
    "            'Comprehensive risk assessment with full context',\n",
    "            'Currency-based laundering pattern detection',\n",
    "            'Institution-level compliance monitoring'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Complete institutional view for comprehensive SAR reporting and cross-bank analysis',\n",
    "        'heterogeneous_features': 'Different feature sets for accounts vs banks with unified dimensionality'\n",
    "    }\n",
    "    \n",
    "    account_list = sorted(node_mappings['accounts'].keys())\n",
    "    bank_list = sorted(node_mappings['banks'].keys())\n",
    "    \n",
    "    node_types = []\n",
    "    node_features_list = []\n",
    "    \n",
    "    account_node_mapping = {}\n",
    "    bank_node_mapping = {}\n",
    "    node_idx = 0\n",
    "    \n",
    "    # Process account nodes\n",
    "    for account in account_list:\n",
    "        account_node_mapping[account] = node_idx\n",
    "        node_types.append(0)  # 0 = account\n",
    "        features = account_features[account]\n",
    "        padded_features = features + [0] * (20 - len(features))\n",
    "        node_features_list.append(padded_features[:20])\n",
    "        node_idx += 1\n",
    "    \n",
    "    # Engineer bank features\n",
    "    bank_features = {}\n",
    "    bank_feature_definitions = {\n",
    "        'total_transactions': 'Total number of transactions involving this bank',\n",
    "        'total_volume': 'Total transaction volume (USD)',\n",
    "        'avg_transaction_amount': 'Average transaction amount for this bank',\n",
    "        'ml_transactions': 'Number of money laundering transactions',\n",
    "        'currency_diversity': 'Number of different currencies handled',\n",
    "        'payment_format_diversity': 'Number of different payment formats supported'\n",
    "    }\n",
    "    \n",
    "    for bank in bank_list:\n",
    "        bank_txns = transactions_df[\n",
    "            (transactions_df['from_bank'] == bank) | \n",
    "            (transactions_df['to_bank'] == bank)\n",
    "        ]\n",
    "        \n",
    "        bank_feature = [\n",
    "            len(bank_txns),\n",
    "            bank_txns['amount_paid'].sum(),\n",
    "            bank_txns['amount_paid'].mean() if len(bank_txns) > 0 else 0,\n",
    "            bank_txns['is_laundering'].sum(),\n",
    "            bank_txns['payment_currency'].nunique(),\n",
    "            bank_txns['payment_format'].nunique()\n",
    "        ]\n",
    "        bank_features[bank] = bank_feature\n",
    "    \n",
    "    # Process bank nodes\n",
    "    for bank in bank_list:\n",
    "        bank_node_mapping[bank] = node_idx\n",
    "        node_types.append(1)  # 1 = bank\n",
    "        features = bank_features[bank]\n",
    "        padded_features = features + [0] * (20 - len(features))\n",
    "        node_features_list.append(padded_features[:20])\n",
    "        node_idx += 1\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    edge_feature_definitions = {\n",
    "        'transaction_amount': 'Amount transferred (0 for institutional relationships)',\n",
    "        'payment_format_encoded': 'Encoded payment format (0 for institutional relationships)',\n",
    "        'currency_encoded': 'Encoded currency (0 for institutional relationships)', \n",
    "        'relationship_type': 'Edge type (0=transfer, 1=institutional_relationship)'\n",
    "    }\n",
    "    \n",
    "    # Add transaction edges (account to account)\n",
    "    for _, txn in transactions_df.iterrows():\n",
    "        if (txn['account_origin'] in account_node_mapping and \n",
    "            txn['account_destination'] in account_node_mapping):\n",
    "            \n",
    "            source_account_idx = account_node_mapping[txn['account_origin']]\n",
    "            target_account_idx = account_node_mapping[txn['account_destination']]\n",
    "            \n",
    "            edge_list.append([source_account_idx, target_account_idx])\n",
    "            edge_attributes.append([\n",
    "                txn['amount_paid'],\n",
    "                payment_encoder.transform([txn['payment_format']])[0],\n",
    "                currency_encoder.transform([txn['payment_currency']])[0],\n",
    "                0  # transfer relationship\n",
    "            ])\n",
    "    \n",
    "    # Add institutional edges (account to bank)\n",
    "    institutional_edges = 0\n",
    "    for _, txn in transactions_df.iterrows():\n",
    "        if txn['account_origin'] in account_node_mapping and txn['from_bank'] in bank_node_mapping:\n",
    "            source_account_idx = account_node_mapping[txn['account_origin']]\n",
    "            from_bank_idx = bank_node_mapping[txn['from_bank']]\n",
    "            edge_list.append([source_account_idx, from_bank_idx])\n",
    "            edge_attributes.append([0, 0, 0, 1])  # institutional relationship\n",
    "            institutional_edges += 1\n",
    "        \n",
    "        if txn['account_destination'] in account_node_mapping and txn['to_bank'] in bank_node_mapping:\n",
    "            target_account_idx = account_node_mapping[txn['account_destination']]\n",
    "            to_bank_idx = bank_node_mapping[txn['to_bank']]\n",
    "            edge_list.append([target_account_idx, to_bank_idx])\n",
    "            edge_attributes.append([0, 0, 0, 1])  # institutional relationship\n",
    "            institutional_edges += 1\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float)\n",
    "    \n",
    "    scaler_multimodal = StandardScaler()\n",
    "    node_features_scaled = scaler_multimodal.fit_transform(node_features_list)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "    node_type = torch.tensor(node_types, dtype=torch.long)\n",
    "    \n",
    "    # Create labels (ML involvement for both accounts and banks)\n",
    "    node_labels = []\n",
    "    for account in account_list:\n",
    "        account_ml_involvement = transactions_df[\n",
    "            (transactions_df['account_origin'] == account) | \n",
    "            (transactions_df['account_destination'] == account)\n",
    "        ]['is_laundering'].sum()\n",
    "        node_labels.append(1 if account_ml_involvement > 0 else 0)\n",
    "    \n",
    "    for bank in bank_list:\n",
    "        bank_ml_involvement = transactions_df[\n",
    "            ((transactions_df['from_bank'] == bank) | \n",
    "             (transactions_df['to_bank'] == bank)) & \n",
    "            (transactions_df['is_laundering'] == 1)\n",
    "        ].shape[0]\n",
    "        node_labels.append(1 if bank_ml_involvement > 0 else 0)\n",
    "    \n",
    "    y = torch.tensor(node_labels, dtype=torch.long)\n",
    "    \n",
    "    multimodal_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type,\n",
    "        num_nodes=len(node_features_list)\n",
    "    )\n",
    "    \n",
    "    graph_stats = {\n",
    "        'total_nodes': multimodal_graph.num_nodes,\n",
    "        'account_nodes': len(account_list),\n",
    "        'bank_nodes': len(bank_list),\n",
    "        'total_edges': multimodal_graph.edge_index.size(1),\n",
    "        'transfer_edges': multimodal_graph.edge_index.size(1) - institutional_edges,\n",
    "        'institutional_edges': institutional_edges,\n",
    "        'node_features': multimodal_graph.x.size(1),\n",
    "        'edge_features': multimodal_graph.edge_attr.size(1),\n",
    "        'ml_entities': y.sum().item(),\n",
    "        'ml_entity_rate': (y.sum().item() / len(node_features_list)),\n",
    "        'heterogeneous_ratio': len(bank_list) / len(account_list)\n",
    "    }\n",
    "    \n",
    "    unified_feature_definitions = {}\n",
    "    for i, feature_name in enumerate(list(feature_definitions.keys())[:17]):\n",
    "        unified_feature_definitions[f'feature_{i:02d}_{feature_name}'] = feature_definitions[feature_name]\n",
    "    for i, feature_name in enumerate(list(bank_feature_definitions.keys())[:6]):\n",
    "        unified_feature_definitions[f'feature_{i+17:02d}_{feature_name}_bank'] = bank_feature_definitions[feature_name] + ' (bank nodes only)'\n",
    "    \n",
    "    graph_annotation.update({\n",
    "        'node_features': unified_feature_definitions,\n",
    "        'edge_features': edge_feature_definitions,\n",
    "        'bank_features': bank_feature_definitions,\n",
    "        'statistics': graph_stats,\n",
    "        'preprocessing': {\n",
    "            'feature_unification': 'Account and bank features padded to common 20-dimensional space',\n",
    "            'node_type_encoding': '0=account, 1=bank for heterogeneous processing',\n",
    "            'edge_type_encoding': '0=transfer, 1=institutional_relationship',\n",
    "            'scaling': 'StandardScaler applied across all node types'\n",
    "        },\n",
    "        'usage_recommendations': {\n",
    "            'training': 'Primary graph for production deployment with full institutional context',\n",
    "            'explanation': 'Complete multi-entity explanations for complex investigations',\n",
    "            'investigation': 'Cross-institutional analysis and bank-level risk assessment',\n",
    "            'validation': 'Comprehensive validation using both account and bank-level ground truth'\n",
    "        },\n",
    "        'heterogeneous_considerations': {\n",
    "            'node_type_handling': 'Use node_type tensor for type-specific processing in GNNs',\n",
    "            'edge_type_handling': 'Filter by relationship_type for different analysis modes',\n",
    "            'feature_interpretation': 'First 17 features are account-based, remaining are bank-based',\n",
    "            'training_strategy': 'Consider separate loss functions for different node types'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['graphs']['multimodal_integration'] = graph_annotation\n",
    "    \n",
    "    print(f\"Multi-Modal Integration Graph created with annotations:\")\n",
    "    print(f\"  Total nodes: {graph_stats['total_nodes']:,}\")\n",
    "    print(f\"  Account nodes: {graph_stats['account_nodes']:,}\")\n",
    "    print(f\"  Bank nodes: {graph_stats['bank_nodes']:,}\")\n",
    "    print(f\"  Transfer edges: {graph_stats['transfer_edges']:,}\")\n",
    "    print(f\"  Institutional edges: {graph_stats['institutional_edges']:,}\")\n",
    "    print(f\"  ML entities: {graph_stats['ml_entities']:,}\")\n",
    "    \n",
    "    return multimodal_graph, scaler_multimodal, account_node_mapping, bank_node_mapping, graph_annotation\n",
    "\n",
    "multimodal_graph, multimodal_scaler, account_node_mapping, bank_node_mapping, mm_annotation = build_annotated_multimodal_integration_graph(\n",
    "    transactions_df, node_mappings, account_features, feature_definitions, payment_encoder, currency_encoder, annotated_constructor\n",
    ")\n",
    "\n",
    "annotated_constructor.graphs['multimodal_integration'] = multimodal_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16549411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Ground Truth Pattern Graph...\")\n",
    "\n",
    "def build_annotated_ground_truth_pattern_graph(pattern_transactions_df, node_mappings, account_features, feature_definitions, annotated_constructor):\n",
    "    \"\"\"Build ground truth pattern graph with comprehensive annotations\"\"\"\n",
    "    \n",
    "    if len(pattern_transactions_df) == 0:\n",
    "        print(\"No pattern transactions available\")\n",
    "        return None, None, None\n",
    "    \n",
    "    graph_annotation = {\n",
    "        'graph_name': 'Ground Truth Pattern Graph',\n",
    "        'graph_type': 'Homogeneous Directed Graph with Pattern Metadata',\n",
    "        'purpose': 'Model validation and explanation quality assessment using labeled money laundering patterns',\n",
    "        'node_type': 'Accounts Involved in Known Money Laundering Patterns',\n",
    "        'edge_type': 'Pattern-Based Transactions with Sequence Information',\n",
    "        'construction_method': 'Subset of accounts and transactions from labeled ML patterns',\n",
    "        'pattern_types_included': list(pattern_transactions_df['pattern_type'].unique()) if len(pattern_transactions_df) > 0 else [],\n",
    "        'key_applications': [\n",
    "            'GNN attention weight validation against known patterns',\n",
    "            'Explanation quality assessment',\n",
    "            'Pattern-specific model performance evaluation',\n",
    "            'Attention mechanism calibration',\n",
    "            'Regulatory explanation validation'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Provides ground truth for validating AI explanations against expert knowledge',\n",
    "        'validation_purpose': 'Critical for ensuring model explanations align with known laundering structures'\n",
    "    }\n",
    "    \n",
    "    pattern_accounts = set()\n",
    "    pattern_accounts.update(pattern_transactions_df['account_origin'].unique())\n",
    "    pattern_accounts.update(pattern_transactions_df['account_destination'].unique())\n",
    "    \n",
    "    pattern_accounts = [acc for acc in pattern_accounts if acc in node_mappings['accounts']]\n",
    "    \n",
    "    if len(pattern_accounts) == 0:\n",
    "        print(\"No pattern accounts found in node mappings\")\n",
    "        return None, None, None\n",
    "    \n",
    "    pattern_account_mapping = {acc: idx for idx, acc in enumerate(pattern_accounts)}\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    pattern_encoder = LabelEncoder()\n",
    "    pattern_types = pattern_transactions_df['pattern_type'].unique()\n",
    "    pattern_encoder.fit(pattern_types)\n",
    "    \n",
    "    edge_feature_definitions = {\n",
    "        'pattern_type_encoded': f'Money laundering pattern type (0-{len(pattern_types)-1}): {list(pattern_types)}',\n",
    "        'transaction_sequence': 'Position of transaction within the pattern (1-based)',\n",
    "        'total_pattern_transactions': 'Total number of transactions in this pattern instance',\n",
    "        'transaction_amount': 'Amount of this specific pattern transaction'\n",
    "    }\n",
    "    \n",
    "    pattern_statistics = {\n",
    "        'total_pattern_types': len(pattern_types),\n",
    "        'pattern_type_distribution': pattern_transactions_df['pattern_type'].value_counts().to_dict(),\n",
    "        'avg_pattern_length': pattern_transactions_df.groupby('pattern_id')['total_txns_in_pattern'].first().mean(),\n",
    "        'total_pattern_instances': pattern_transactions_df['pattern_id'].nunique()\n",
    "    }\n",
    "    \n",
    "    for _, txn in pattern_transactions_df.iterrows():\n",
    "        if (txn['account_origin'] in pattern_account_mapping and \n",
    "            txn['account_destination'] in pattern_account_mapping):\n",
    "            \n",
    "            source_idx = pattern_account_mapping[txn['account_origin']]\n",
    "            target_idx = pattern_account_mapping[txn['account_destination']]\n",
    "            \n",
    "            edge_list.append([source_idx, target_idx])\n",
    "            edge_attributes.append([\n",
    "                pattern_encoder.transform([txn['pattern_type']])[0],\n",
    "                txn['txn_sequence'],\n",
    "                txn['total_txns_in_pattern'],\n",
    "                txn['amount_paid']\n",
    "            ])\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        print(\"No edges found in pattern graph\")\n",
    "        return None, None, None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attributes, dtype=torch.float)\n",
    "    \n",
    "    node_features = []\n",
    "    for account in pattern_accounts:\n",
    "        features = account_features[account]\n",
    "        node_features.append(features)\n",
    "    \n",
    "    scaler_pattern = StandardScaler()\n",
    "    node_features_scaled = scaler_pattern.fit_transform(node_features)\n",
    "    \n",
    "    x = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "    y = torch.ones(len(pattern_accounts), dtype=torch.long)  # All nodes are ML-involved\n",
    "    \n",
    "    pattern_graph = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        num_nodes=len(pattern_accounts)\n",
    "    )\n",
    "    \n",
    "    graph_stats = {\n",
    "        'nodes': pattern_graph.num_nodes,\n",
    "        'edges': pattern_graph.edge_index.size(1),\n",
    "        'node_features': pattern_graph.x.size(1),\n",
    "        'edge_features': pattern_graph.edge_attr.size(1),\n",
    "        'pattern_types': len(pattern_types),\n",
    "        'ml_label_rate': 1.0,  # All accounts are ML-involved by definition\n",
    "        'avg_degree': (edge_index.size(1) / len(pattern_accounts)),\n",
    "        'pattern_coverage': len(pattern_accounts) / len(node_mappings['accounts'])\n",
    "    }\n",
    "    \n",
    "    graph_annotation.update({\n",
    "        'node_features': feature_definitions,\n",
    "        'edge_features': edge_feature_definitions,\n",
    "        'pattern_statistics': pattern_statistics,\n",
    "        'statistics': graph_stats,\n",
    "        'preprocessing': {\n",
    "            'account_filtering': 'Only accounts involved in labeled patterns included',\n",
    "            'pattern_encoding': 'LabelEncoder for pattern types with sequence preservation',\n",
    "            'feature_consistency': 'Same behavioral features as other graphs for comparison'\n",
    "        },\n",
    "        'usage_recommendations': {\n",
    "            'training': 'Use for specialized pattern-aware training and validation',\n",
    "            'explanation': 'Gold standard for validating attention weight quality',\n",
    "            'investigation': 'Reference for understanding known pattern structures',\n",
    "            'validation': 'Primary validation graph for explanation quality assessment'\n",
    "        },\n",
    "        'validation_applications': {\n",
    "            'attention_validation': 'Compare GAT attention weights with known pattern edges',\n",
    "            'explanation_quality': 'Measure explanation coverage of known patterns',\n",
    "            'pattern_detection': 'Verify model can identify different pattern types',\n",
    "            'sequence_analysis': 'Validate temporal sequence understanding'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['graphs']['ground_truth_patterns'] = graph_annotation\n",
    "    \n",
    "    print(f\"Ground Truth Pattern Graph created with annotations:\")\n",
    "    print(f\"  Nodes: {graph_stats['nodes']:,}\")\n",
    "    print(f\"  Edges: {graph_stats['edges']:,}\")\n",
    "    print(f\"  Pattern types: {graph_stats['pattern_types']}\")\n",
    "    print(f\"  Pattern coverage: {graph_stats['pattern_coverage']:.4f}\")\n",
    "    \n",
    "    return pattern_graph, pattern_account_mapping, graph_annotation\n",
    "\n",
    "pattern_graph, pattern_account_mapping, pt_annotation = build_annotated_ground_truth_pattern_graph(\n",
    "    pattern_transactions_df, node_mappings, account_features, feature_definitions, annotated_constructor\n",
    ")\n",
    "\n",
    "if pattern_graph is not None:\n",
    "    annotated_constructor.graphs['ground_truth_patterns'] = pattern_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building annotated Ego-Network Extraction Framework...\")\n",
    "\n",
    "def create_annotated_ego_network_extractor(transaction_flow_graph, node_mappings, annotated_constructor, k_hops=2):\n",
    "    \"\"\"Create ego-network extractor with comprehensive annotations\"\"\"\n",
    "    \n",
    "    framework_annotation = {\n",
    "        'framework_name': 'Ego-Network Extraction System',\n",
    "        'purpose': 'Focused subgraph extraction for real-time investigation and explanation',\n",
    "        'extraction_method': f'{k_hops}-hop neighborhood extraction around target accounts',\n",
    "        'k_hops': k_hops,\n",
    "        'source_graph': 'Transaction Flow Graph',\n",
    "        'key_applications': [\n",
    "            'Real-time focused investigation around suspicious accounts',\n",
    "            'Manageable subgraph visualization for compliance teams',\n",
    "            'Detailed explanation generation for specific accounts',\n",
    "            'Local pattern analysis within account neighborhoods',\n",
    "            'Scalable analysis of large transaction networks'\n",
    "        ],\n",
    "        'regulatory_compliance': 'Enables focused analysis for specific account investigations in SARs',\n",
    "        'computational_efficiency': 'Reduces graph complexity for real-time processing'\n",
    "    }\n",
    "    \n",
    "    def extract_annotated_ego_network(target_account, k=k_hops):\n",
    "        \"\"\"Extract ego-network with detailed annotations\"\"\"\n",
    "        \n",
    "        if target_account not in node_mappings['accounts']:\n",
    "            return None\n",
    "        \n",
    "        target_idx = node_mappings['accounts'][target_account]\n",
    "        \n",
    "        edge_index = transaction_flow_graph.edge_index\n",
    "        current_nodes = {target_idx}\n",
    "        hop_nodes = {0: {target_idx}}\n",
    "        \n",
    "        # Multi-hop expansion with tracking\n",
    "        for hop in range(k):\n",
    "            next_nodes = set()\n",
    "            \n",
    "            for node in current_nodes:\n",
    "                neighbors_out = edge_index[1][edge_index[0] == node].tolist()\n",
    "                neighbors_in = edge_index[0][edge_index[1] == node].tolist()\n",
    "                next_nodes.update(neighbors_out + neighbors_in)\n",
    "            \n",
    "            new_nodes = next_nodes - current_nodes\n",
    "            hop_nodes[hop + 1] = new_nodes\n",
    "            current_nodes.update(next_nodes)\n",
    "        \n",
    "        ego_nodes = sorted(list(current_nodes))\n",
    "        \n",
    "        if len(ego_nodes) <= 1:\n",
    "            return None\n",
    "        \n",
    "        node_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(ego_nodes)}\n",
    "        \n",
    "        ego_edge_list = []\n",
    "        ego_edge_attrs = []\n",
    "        edge_metadata = []\n",
    "        \n",
    "        for i, (src, dst) in enumerate(edge_index.t().tolist()):\n",
    "            if src in node_mapping and dst in node_mapping:\n",
    "                ego_edge_list.append([node_mapping[src], node_mapping[dst]])\n",
    "                ego_edge_attrs.append(transaction_flow_graph.edge_attr[i].tolist())\n",
    "                \n",
    "                # Add edge metadata for explanation\n",
    "                src_hop = None\n",
    "                dst_hop = None\n",
    "                for hop, nodes in hop_nodes.items():\n",
    "                    if src in nodes:\n",
    "                        src_hop = hop\n",
    "                    if dst in nodes:\n",
    "                        dst_hop = hop\n",
    "                \n",
    "                edge_metadata.append({\n",
    "                    'source_hop': src_hop,\n",
    "                    'target_hop': dst_hop,\n",
    "                    'edge_type': 'internal' if src_hop == dst_hop else 'cross_hop'\n",
    "                })\n",
    "        \n",
    "        if len(ego_edge_list) == 0:\n",
    "            return None\n",
    "        \n",
    "        ego_edge_index = torch.tensor(ego_edge_list, dtype=torch.long).t().contiguous()\n",
    "        ego_edge_attr = torch.tensor(ego_edge_attrs, dtype=torch.float)\n",
    "        \n",
    "        ego_x = transaction_flow_graph.x[ego_nodes]\n",
    "        ego_y = transaction_flow_graph.y[ego_nodes]\n",
    "        \n",
    "        # Create hop-based node annotations\n",
    "        node_hop_labels = torch.zeros(len(ego_nodes), dtype=torch.long)\n",
    "        for hop, nodes in hop_nodes.items():\n",
    "            for node_idx in nodes:\n",
    "                if node_idx in node_mapping:\n",
    "                    node_hop_labels[node_mapping[node_idx]] = hop\n",
    "        \n",
    "        ego_graph = Data(\n",
    "            x=ego_x,\n",
    "            edge_index=ego_edge_index,\n",
    "            edge_attr=ego_edge_attr,\n",
    "            y=ego_y,\n",
    "            hop_labels=node_hop_labels,\n",
    "            num_nodes=len(ego_nodes)\n",
    "        )\n",
    "        \n",
    "        # Create extraction metadata\n",
    "        extraction_metadata = {\n",
    "            'target_account': target_account,\n",
    "            'target_node_idx': target_idx,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'k_hops': k,\n",
    "            'total_nodes_extracted': len(ego_nodes),\n",
    "            'nodes_by_hop': {hop: len(nodes) for hop, nodes in hop_nodes.items()},\n",
    "            'total_edges_extracted': ego_edge_index.size(1),\n",
    "            'ml_nodes_in_ego': ego_y.sum().item(),\n",
    "            'ml_rate_in_ego': ego_y.float().mean().item(),\n",
    "            'coverage_ratio': len(ego_nodes) / transaction_flow_graph.num_nodes,\n",
    "            'hop_distribution': node_hop_labels.bincount().tolist(),\n",
    "            'edge_metadata': edge_metadata\n",
    "        }\n",
    "        \n",
    "        return ego_graph, ego_nodes, node_mapping, extraction_metadata\n",
    "    \n",
    "    # Test extraction system\n",
    "    ml_accounts = transactions_df[transactions_df['is_laundering'] == 1]['account_origin'].unique()[:5]\n",
    "    \n",
    "    sample_ego_networks = {}\n",
    "    extraction_stats = {\n",
    "        'total_extractions': 0,\n",
    "        'successful_extractions': 0,\n",
    "        'avg_nodes_per_ego': 0,\n",
    "        'avg_edges_per_ego': 0,\n",
    "        'avg_ml_rate_per_ego': 0\n",
    "    }\n",
    "    \n",
    "    print(\"Testing ego-network extraction on ML accounts...\")\n",
    "    \n",
    "    total_nodes = 0\n",
    "    total_edges = 0\n",
    "    total_ml_rates = 0\n",
    "    \n",
    "    for account in ml_accounts:\n",
    "        extraction_stats['total_extractions'] += 1\n",
    "        ego_result = extract_annotated_ego_network(account)\n",
    "        \n",
    "        if ego_result is not None:\n",
    "            ego_graph, ego_nodes, ego_mapping, ego_metadata = ego_result\n",
    "            sample_ego_networks[account] = {\n",
    "                'graph': ego_graph,\n",
    "                'metadata': ego_metadata\n",
    "            }\n",
    "            extraction_stats['successful_extractions'] += 1\n",
    "            total_nodes += ego_graph.num_nodes\n",
    "            total_edges += ego_graph.edge_index.size(1)\n",
    "            total_ml_rates += ego_metadata['ml_rate_in_ego']\n",
    "            \n",
    "            print(f\"  Account {account}: {ego_graph.num_nodes} nodes, {ego_graph.edge_index.size(1)} edges, ML rate: {ego_metadata['ml_rate_in_ego']:.3f}\")\n",
    "    \n",
    "    if extraction_stats['successful_extractions'] > 0:\n",
    "        extraction_stats['avg_nodes_per_ego'] = total_nodes / extraction_stats['successful_extractions']\n",
    "        extraction_stats['avg_edges_per_ego'] = total_edges / extraction_stats['successful_extractions']\n",
    "        extraction_stats['avg_ml_rate_per_ego'] = total_ml_rates / extraction_stats['successful_extractions']\n",
    "    \n",
    "    framework_annotation.update({\n",
    "        'extraction_function': extract_annotated_ego_network,\n",
    "        'sample_extractions': len(sample_ego_networks),\n",
    "        'extraction_statistics': extraction_stats,\n",
    "        'usage_recommendations': {\n",
    "            'real_time_investigation': 'Extract ego-networks for suspicious accounts during investigation',\n",
    "            'explanation_generation': 'Use for focused, interpretable explanations',\n",
    "            'visualization': 'Create manageable network visualizations for compliance teams',\n",
    "            'validation': 'Compare model attention with ego-network structure'\n",
    "        },\n",
    "        'output_components': {\n",
    "            'ego_graph': 'PyTorch Geometric Data object with subgraph',\n",
    "            'ego_nodes': 'List of original node indices in ego-network',\n",
    "            'node_mapping': 'Mapping from original to ego-network indices',\n",
    "            'extraction_metadata': 'Detailed metadata about extraction process and results'\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    annotated_constructor.annotations['frameworks'] = annotated_constructor.annotations.get('frameworks', {})\n",
    "    annotated_constructor.annotations['frameworks']['ego_network_extraction'] = framework_annotation\n",
    "    \n",
    "    return extract_annotated_ego_network, sample_ego_networks, framework_annotation\n",
    "\n",
    "ego_network_extractor, sample_ego_networks, ego_annotation = create_annotated_ego_network_extractor(\n",
    "    transaction_flow_graph, node_mappings, annotated_constructor, k_hops=2\n",
    ")\n",
    "\n",
    "annotated_constructor.graphs['ego_networks'] = sample_ego_networks\n",
    "print(f\"Created annotated ego-network extractor with {len(sample_ego_networks)} sample networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotated_graphs_with_metadata(annotated_constructor):\n",
    "    \"\"\"Save all graphs and annotations with comprehensive metadata\"\"\"\n",
    "    \n",
    "    # Save graphs with proper error handling\n",
    "    graph_save_data = {}\n",
    "    \n",
    "    # Check and save each graph individually\n",
    "    if 'transaction_flow' in annotated_constructor.graphs and annotated_constructor.graphs['transaction_flow'] is not None:\n",
    "        graph_save_data['transaction_flow_graph'] = annotated_constructor.graphs['transaction_flow']\n",
    "    \n",
    "    if 'temporal_proximity' in annotated_constructor.graphs and annotated_constructor.graphs['temporal_proximity'] is not None:\n",
    "        graph_save_data['temporal_proximity_graph'] = annotated_constructor.graphs['temporal_proximity']\n",
    "    \n",
    "    if 'account_behavior' in annotated_constructor.graphs and annotated_constructor.graphs['account_behavior'] is not None:\n",
    "        graph_save_data['account_behavior_graph'] = annotated_constructor.graphs['account_behavior']\n",
    "    \n",
    "    if 'multimodal_integration' in annotated_constructor.graphs and annotated_constructor.graphs['multimodal_integration'] is not None:\n",
    "        graph_save_data['multimodal_integration_graph'] = annotated_constructor.graphs['multimodal_integration']\n",
    "    \n",
    "    if 'ground_truth_patterns' in annotated_constructor.graphs and annotated_constructor.graphs['ground_truth_patterns'] is not None:\n",
    "        graph_save_data['ground_truth_pattern_graph'] = annotated_constructor.graphs['ground_truth_patterns']\n",
    "    \n",
    "    if 'ego_networks' in annotated_constructor.graphs and annotated_constructor.graphs['ego_networks'] is not None:\n",
    "        graph_save_data['sample_ego_networks'] = annotated_constructor.graphs['ego_networks']\n",
    "    \n",
    "    torch.save(graph_save_data, 'data/annotated_graphs.pt')\n",
    "    \n",
    "    # Save preprocessors with metadata - need to handle globals properly\n",
    "    preprocessor_save_data = {\n",
    "        'preprocessor_metadata': {}\n",
    "    }\n",
    "    \n",
    "    # Only save preprocessors that exist\n",
    "    if 'feature_scaler' in globals() and feature_scaler is not None:\n",
    "        preprocessor_save_data['feature_scaler'] = feature_scaler\n",
    "        preprocessor_save_data['preprocessor_metadata']['feature_scaler_info'] = {\n",
    "            'type': 'StandardScaler',\n",
    "            'applied_to': 'Account behavioral features',\n",
    "            'feature_count': len(feature_names) if 'feature_names' in globals() else 0,\n",
    "            'mean_values': feature_scaler.mean_.tolist(),\n",
    "            'scale_values': feature_scaler.scale_.tolist()\n",
    "        }\n",
    "    \n",
    "    if 'temporal_scaler' in globals() and temporal_scaler is not None:\n",
    "        preprocessor_save_data['temporal_scaler'] = temporal_scaler\n",
    "        preprocessor_save_data['preprocessor_metadata']['temporal_scaler_info'] = {\n",
    "            'type': 'StandardScaler', \n",
    "            'applied_to': 'Transaction-level features for temporal graph',\n",
    "            'feature_count': temporal_scaler.n_features_in_\n",
    "        }\n",
    "    \n",
    "    if 'behavior_scaler' in globals() and behavior_scaler is not None:\n",
    "        preprocessor_save_data['behavior_scaler'] = behavior_scaler\n",
    "        preprocessor_save_data['preprocessor_metadata']['behavior_scaler_info'] = {\n",
    "            'type': 'StandardScaler',\n",
    "            'applied_to': 'Behavioral features for similarity computation',\n",
    "            'feature_count': behavior_scaler.n_features_in_\n",
    "        }\n",
    "    \n",
    "    if 'multimodal_scaler' in globals() and multimodal_scaler is not None:\n",
    "        preprocessor_save_data['multimodal_scaler'] = multimodal_scaler\n",
    "        preprocessor_save_data['preprocessor_metadata']['multimodal_scaler_info'] = {\n",
    "            'type': 'StandardScaler',\n",
    "            'applied_to': 'Unified account and bank features',\n",
    "            'feature_count': multimodal_scaler.n_features_in_\n",
    "        }\n",
    "    \n",
    "    if 'payment_encoder' in globals() and payment_encoder is not None:\n",
    "        preprocessor_save_data['payment_encoder'] = payment_encoder\n",
    "        preprocessor_save_data['preprocessor_metadata']['payment_encoder_info'] = {\n",
    "            'type': 'LabelEncoder',\n",
    "            'classes': payment_encoder.classes_.tolist(),\n",
    "            'class_count': len(payment_encoder.classes_)\n",
    "        }\n",
    "    \n",
    "    if 'currency_encoder' in globals() and currency_encoder is not None:\n",
    "        preprocessor_save_data['currency_encoder'] = currency_encoder\n",
    "        preprocessor_save_data['preprocessor_metadata']['currency_encoder_info'] = {\n",
    "            'type': 'LabelEncoder',\n",
    "            'classes': currency_encoder.classes_.tolist(),\n",
    "            'class_count': len(currency_encoder.classes_)\n",
    "        }\n",
    "    \n",
    "    torch.save(preprocessor_save_data, 'data/annotated_preprocessors.pt')\n",
    "    \n",
    "    # Save comprehensive metadata with proper error handling\n",
    "    comprehensive_metadata = {\n",
    "        'project_metadata': annotated_constructor.annotations if hasattr(annotated_constructor, 'annotations') else {},\n",
    "        'creation_info': {\n",
    "            'creation_timestamp': annotated_constructor.creation_timestamp if hasattr(annotated_constructor, 'creation_timestamp') else datetime.now().isoformat(),\n",
    "            'version': annotated_constructor.version if hasattr(annotated_constructor, 'version') else \"1.0\",\n",
    "            'total_graphs_created': len(annotated_constructor.graphs),\n",
    "            'data_sources_processed': len(annotated_constructor.annotations.get('data_sources', {})) if hasattr(annotated_constructor, 'annotations') else 0,\n",
    "            'total_annotations': sum(len(v) if isinstance(v, dict) else 1 for v in annotated_constructor.annotations.values()) if hasattr(annotated_constructor, 'annotations') else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add other metadata if variables exist in global scope\n",
    "    if 'node_mappings' in globals():\n",
    "        comprehensive_metadata['node_mappings'] = node_mappings\n",
    "    \n",
    "    if 'account_features' in globals():\n",
    "        comprehensive_metadata['account_features'] = account_features\n",
    "    \n",
    "    if 'feature_names' in globals():\n",
    "        comprehensive_metadata['feature_names'] = feature_names\n",
    "    \n",
    "    if 'feature_definitions' in globals():\n",
    "        comprehensive_metadata['feature_definitions'] = feature_definitions\n",
    "    \n",
    "    if 'account_node_mapping' in globals():\n",
    "        comprehensive_metadata['account_node_mapping'] = account_node_mapping\n",
    "    \n",
    "    if 'bank_node_mapping' in globals():\n",
    "        comprehensive_metadata['bank_node_mapping'] = bank_node_mapping\n",
    "    \n",
    "    if 'pattern_account_mapping' in globals():\n",
    "        comprehensive_metadata['pattern_account_mapping'] = pattern_account_mapping\n",
    "    elif 'pattern_graph' in globals() and pattern_graph is not None:\n",
    "        comprehensive_metadata['pattern_account_mapping'] = None\n",
    "    \n",
    "    with open('data/comprehensive_graph_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(comprehensive_metadata, f)\n",
    "    \n",
    "    # Save JSON version for human readability\n",
    "    try:\n",
    "        json_serializable_annotations = convert_to_json_serializable(annotated_constructor.annotations)\n",
    "        with open('data/graph_annotations.json', 'w') as f:\n",
    "            json.dump(json_serializable_annotations, f, indent=2, default=str)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save JSON annotations: {e}\")\n",
    "        # Create minimal JSON file\n",
    "        minimal_annotations = {\n",
    "            'project_info': {\n",
    "                'name': 'Explainable AML Detection with Graph Neural Networks',\n",
    "                'version': '1.0',\n",
    "                'creation_date': datetime.now().isoformat(),\n",
    "                'status': 'Stage 3 Completed'\n",
    "            },\n",
    "            'graphs_created': list(graph_save_data.keys()),\n",
    "            'total_graphs': len(graph_save_data)\n",
    "        }\n",
    "        with open('data/graph_annotations.json', 'w') as f:\n",
    "            json.dump(minimal_annotations, f, indent=2, default=str)\n",
    "    \n",
    "    return {\n",
    "        'graphs_saved': 'data/annotated_graphs.pt',\n",
    "        'preprocessors_saved': 'data/annotated_preprocessors.pt',\n",
    "        'metadata_saved': 'data/comprehensive_graph_metadata.pkl',\n",
    "        'annotations_saved': 'data/graph_annotations.json',\n",
    "        'graphs_count': len(graph_save_data),\n",
    "        'preprocessors_count': len([k for k in preprocessor_save_data.keys() if k != 'preprocessor_metadata'])\n",
    "    }\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert complex objects to JSON-serializable format\"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (int, float, str, bool)):\n",
    "        return obj\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except:\n",
    "            return \"<<non-serializable object>>\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
